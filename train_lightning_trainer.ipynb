{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atomwalk12/Dropbox (Old)/notes/vision/project/BeyondVisionQA/model.py:128: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.metric = load_metric(\"bertscore\")\n",
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for bertscore contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/bertscore/bertscore.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a78df4b0a844c4b16f0f4a427954c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88f12b59396470ab31137269b7e43a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from qadataset import QADataset\n",
    "from config import dataset_config, model_config\n",
    "\n",
    "MODEL_ID = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "\n",
    "train_dataset = QADataset(dataset_config, split=\"train[:100]\")\n",
    "val_dataset = QADataset(dataset_config, split=\"train[:20]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4954627776d4eb0a7e7497f7c964f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from trainer_blip2 import BLIP2ModelPLModule\n",
    "from trainer_blip2 import BLIP2PLModule\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from trainer_blip2 import BLIP2ModelPLModule\n",
    "from trainer_blip2 import BLIP2PLModule\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "\n",
    "def get_model():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=\"all-linear\",\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "model = get_model()\n",
    "# processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"ybelkada/blip2-opt-2.7b-fp16-sharded\", device_map=\"auto\", load_in_8bit=True)\n",
    "# # Let's define the LoraConfig\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     target_modules=[\"q_proj\", \"k_proj\"]\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, collate_fn=BLIP2PLModule.train_textual_labels, batch_size=8, shuffle=True, num_workers=4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from config import dataset_config\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from config import metrics\n",
    "from transformers import AutoProcessor\n",
    "from dataset_configs.easy_vqa import translate\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right\n",
    "\n",
    "class BLIP2ModelPLModule(L.LightningModule):\n",
    "    def __init__(self, hyperparameters, model, train_dataset, val_dataset):\n",
    "        super().__init__()\n",
    "        self.hyperparams = hyperparameters\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "        self.batch_size = hyperparameters.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        inputs, labels = batch\n",
    "\n",
    "        outputs = self.model(**inputs,\n",
    "                            labels=labels)\n",
    "        loss = outputs.loss\n",
    "        print(f\"Epoch {self.current_epoch}, loss: {loss.item()}\")\n",
    "\n",
    "        self.log(\"train_loss\", loss, batch_size=self.batch_size)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "\n",
    "        inputs, answers = batch\n",
    "\n",
    "        # auto-regressively generate token IDs\n",
    "\n",
    "        \n",
    "        generated_ids = self.model.generate(**inputs)\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        print(f\"==================Dataset batch: {batch_idx}/{self.val_dataset.dataset_length // self.batch_size}==================\")\n",
    "        scores = []\n",
    "        i = 0\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            print(f\"Question: {self.val_dataset.dataset[batch_idx*self.batch_size+i]['question']}\")\n",
    "            print(f\"Prediction: {pred}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            i += 1\n",
    "\n",
    "        for metric in metrics:\n",
    "            scores = metric.compute(predictions=predictions, references=answers, model=self)\n",
    "            \n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "       return torch.optim.Adam(self.parameters(), lr=5e-4)\n",
    "        \n",
    "\n",
    "\n",
    "class BLIP2PLModule(BLIP2ModelPLModule):\n",
    "    \n",
    "    def __init__(self, config, model, train_dataset, val_dataset):\n",
    "        super().__init__(config, model, train_dataset, val_dataset)\n",
    "    \n",
    "    def train_numeric_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        batch_labels = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input, label = translate(ground_truth, training=True)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input)\n",
    "            batch_labels.append({ 'label_ids': label['label_ids'], 'scores': torch.from_numpy(label['scores'])})\n",
    "\n",
    "        # inputs = processor(images=images, text=texts, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
    "        inputs = processor(text=texts, images=images, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        result = []\n",
    "\n",
    "        for label in batch_labels:\n",
    "            scores = label['scores']\n",
    "            result.append(scores)\n",
    "        \n",
    "        return inputs, torch.stack(result)\n",
    "\n",
    "\n",
    "    \n",
    "    def train_textual_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input = translate(ground_truth, training=True)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input)    \n",
    "\n",
    "        # inputs = processor(images=images, text=texts, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
    "        inputs = processor(text=texts, images=images, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def eval_numeric_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        answers = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input, output = translate(ground_truth, training=False)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input) \n",
    "            answers.append(output)\n",
    "\n",
    "        inputs = processor(text=texts, images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        return inputs, answers\n",
    "    \n",
    "    def eval_textual_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        answers = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input, output = translate(ground_truth, training=False)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input) \n",
    "            answers.append(output)\n",
    "\n",
    "        inputs = processor(text=texts, images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        return inputs, answers\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        if model_config['classification']:\n",
    "            return DataLoader(self.train_dataset, collate_fn=BLIP2PLModule.train_numeric_labels, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "        else:\n",
    "            return DataLoader(self.train_dataset, collate_fn=BLIP2PLModule.train_textual_labels, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if model_config['classification']:\n",
    "            return DataLoader(self.val_dataset, collate_fn=BLIP2PLModule.eval_numeric_labels, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        else:\n",
    "            return DataLoader(self.val_dataset, collate_fn=BLIP2PLModule.eval_textual_labels, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "hyperparameters = model_config['hyperparameters']\n",
    "if model_config['target'] == 'blip2':\n",
    "    module = BLIP2PLModule(hyperparameters, model, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | PeftModel | 2.0 B  | train\n",
      "--------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "1.9 B     Non-trainable params\n",
      "2.0 B     Total params\n",
      "7,848.709 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([20, 1.67, 1, 1.0, 8, 0.0005, 9, 1337, 1, './result', True, [0.9, 0.999], 0.05, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f51eabcea6246f9a791686421fb287f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 6.874007701873779\n",
      "Epoch 0, loss: 5.443212032318115\n",
      "Epoch 0, loss: 5.726525783538818\n",
      "Epoch 0, loss: 5.582296848297119\n",
      "Epoch 0, loss: 6.08299446105957\n",
      "Epoch 0, loss: 5.11095666885376\n",
      "Epoch 0, loss: 5.314000129699707\n",
      "Epoch 0, loss: 5.9298505783081055\n",
      "Epoch 0, loss: 5.438870906829834\n",
      "Epoch 0, loss: 5.404477596282959\n",
      "Epoch 0, loss: 5.6983466148376465\n",
      "Epoch 0, loss: 4.121647357940674\n",
      "Epoch 1, loss: 3.8298161029815674\n",
      "Epoch 1, loss: 3.3930065631866455\n",
      "Epoch 1, loss: 2.7952518463134766\n",
      "Epoch 1, loss: 4.966162204742432\n",
      "Epoch 1, loss: 3.537862539291382\n",
      "Epoch 1, loss: 3.0506811141967773\n",
      "Epoch 1, loss: 3.5888826847076416\n",
      "Epoch 1, loss: 3.422295093536377\n",
      "Epoch 1, loss: 2.253948926925659\n",
      "Epoch 1, loss: 1.9641398191452026\n",
      "Epoch 1, loss: 2.2987654209136963\n",
      "Epoch 1, loss: 1.2863011360168457\n",
      "Epoch 2, loss: 2.6830484867095947\n",
      "Epoch 2, loss: 2.8315353393554688\n",
      "Epoch 2, loss: 2.2140371799468994\n",
      "Epoch 2, loss: 2.3414504528045654\n",
      "Epoch 2, loss: 2.540097951889038\n",
      "Epoch 2, loss: 2.0032527446746826\n",
      "Epoch 2, loss: 1.784340500831604\n",
      "Epoch 2, loss: 2.0859875679016113\n",
      "Epoch 2, loss: 1.7282418012619019\n",
      "Epoch 2, loss: 1.4819382429122925\n",
      "Epoch 2, loss: 1.5999685525894165\n",
      "Epoch 2, loss: 0.7392447590827942\n",
      "Epoch 3, loss: 1.4265644550323486\n",
      "Epoch 3, loss: 1.090790033340454\n",
      "Epoch 3, loss: 1.390864610671997\n",
      "Epoch 3, loss: 1.238564133644104\n",
      "Epoch 3, loss: 1.595995306968689\n",
      "Epoch 3, loss: 1.3866040706634521\n",
      "Epoch 3, loss: 1.3736733198165894\n",
      "Epoch 3, loss: 1.2399526834487915\n",
      "Epoch 3, loss: 0.8593491315841675\n",
      "Epoch 3, loss: 0.9857056140899658\n",
      "Epoch 3, loss: 1.074090600013733\n",
      "Epoch 3, loss: 0.9856525659561157\n",
      "Epoch 4, loss: 0.9562581181526184\n",
      "Epoch 4, loss: 0.922775387763977\n",
      "Epoch 4, loss: 0.7591586709022522\n",
      "Epoch 4, loss: 0.8681363463401794\n",
      "Epoch 4, loss: 0.8868346214294434\n",
      "Epoch 4, loss: 0.901066243648529\n",
      "Epoch 4, loss: 0.7949934005737305\n",
      "Epoch 4, loss: 1.0008056163787842\n",
      "Epoch 4, loss: 0.7448623180389404\n",
      "Epoch 4, loss: 0.7325421571731567\n",
      "Epoch 4, loss: 0.6672961115837097\n",
      "Epoch 4, loss: 0.23148201406002045\n",
      "Epoch 5, loss: 0.5397024750709534\n",
      "Epoch 5, loss: 0.5099695920944214\n",
      "Epoch 5, loss: 0.6698175668716431\n",
      "Epoch 5, loss: 0.6195054650306702\n",
      "Epoch 5, loss: 0.6578038334846497\n",
      "Epoch 5, loss: 0.5756816267967224\n",
      "Epoch 5, loss: 0.5197516083717346\n",
      "Epoch 5, loss: 0.5459822416305542\n",
      "Epoch 5, loss: 0.5290093421936035\n",
      "Epoch 5, loss: 0.4447862207889557\n",
      "Epoch 5, loss: 0.4533778727054596\n",
      "Epoch 5, loss: 0.5330991148948669\n",
      "Epoch 6, loss: 0.644061803817749\n",
      "Epoch 6, loss: 0.5629000663757324\n",
      "Epoch 6, loss: 0.5244231224060059\n",
      "Epoch 6, loss: 0.5613619089126587\n",
      "Epoch 6, loss: 0.5162415504455566\n",
      "Epoch 6, loss: 0.3601284921169281\n",
      "Epoch 6, loss: 0.4851016104221344\n",
      "Epoch 6, loss: 0.4382310211658478\n",
      "Epoch 6, loss: 0.42177876830101013\n",
      "Epoch 6, loss: 0.39372265338897705\n",
      "Epoch 6, loss: 0.3475582003593445\n",
      "Epoch 6, loss: 0.44611769914627075\n",
      "Epoch 7, loss: 0.45513850450515747\n",
      "Epoch 7, loss: 0.42144379019737244\n",
      "Epoch 7, loss: 0.46998652815818787\n",
      "Epoch 7, loss: 0.3675173223018646\n",
      "Epoch 7, loss: 0.42113614082336426\n",
      "Epoch 7, loss: 0.40915408730506897\n",
      "Epoch 7, loss: 0.4140733778476715\n",
      "Epoch 7, loss: 0.3937993347644806\n",
      "Epoch 7, loss: 0.3499903082847595\n",
      "Epoch 7, loss: 0.4327397346496582\n",
      "Epoch 7, loss: 0.33148476481437683\n",
      "Epoch 7, loss: 0.34288275241851807\n",
      "Epoch 8, loss: 0.341922402381897\n",
      "Epoch 8, loss: 0.39669960737228394\n",
      "Epoch 8, loss: 0.34573379158973694\n",
      "Epoch 8, loss: 0.3660034239292145\n",
      "Epoch 8, loss: 0.4083942472934723\n",
      "Epoch 8, loss: 0.32479631900787354\n",
      "Epoch 8, loss: 0.358829140663147\n",
      "Epoch 8, loss: 0.31321001052856445\n",
      "Epoch 8, loss: 0.35691651701927185\n",
      "Epoch 8, loss: 0.29701781272888184\n",
      "Epoch 8, loss: 0.2903103828430176\n",
      "Epoch 8, loss: 0.4021293520927429\n",
      "Epoch 9, loss: 0.36915525794029236\n",
      "Epoch 9, loss: 0.32398319244384766\n",
      "Epoch 9, loss: 0.25873157382011414\n",
      "Epoch 9, loss: 0.31238505244255066\n",
      "Epoch 9, loss: 0.3383702039718628\n",
      "Epoch 9, loss: 0.34258192777633667\n",
      "Epoch 9, loss: 0.32147377729415894\n",
      "Epoch 9, loss: 0.3102565407752991\n",
      "Epoch 9, loss: 0.30189624428749084\n",
      "Epoch 9, loss: 0.3434011936187744\n",
      "Epoch 9, loss: 0.2878563702106476\n",
      "Epoch 9, loss: 0.48455527424812317\n",
      "Epoch 10, loss: 0.258489191532135\n",
      "Epoch 10, loss: 0.31921178102493286\n",
      "Epoch 10, loss: 0.3050295114517212\n",
      "Epoch 10, loss: 0.29683250188827515\n",
      "Epoch 10, loss: 0.32033196091651917\n",
      "Epoch 10, loss: 0.38509830832481384\n",
      "Epoch 10, loss: 0.35543712973594666\n",
      "Epoch 10, loss: 0.30838653445243835\n",
      "Epoch 10, loss: 0.2698640823364258\n",
      "Epoch 10, loss: 0.33683139085769653\n",
      "Epoch 10, loss: 0.3026566803455353\n",
      "Epoch 10, loss: 0.4801027178764343\n",
      "Epoch 11, loss: 0.33827269077301025\n",
      "Epoch 11, loss: 0.37001949548721313\n",
      "Epoch 11, loss: 0.32789403200149536\n",
      "Epoch 11, loss: 0.3619651198387146\n",
      "Epoch 11, loss: 0.3499637842178345\n",
      "Epoch 11, loss: 0.3857514560222626\n",
      "Epoch 11, loss: 0.334323912858963\n",
      "Epoch 11, loss: 0.3353686034679413\n",
      "Epoch 11, loss: 0.3217015862464905\n",
      "Epoch 11, loss: 0.34385332465171814\n",
      "Epoch 11, loss: 0.3436577022075653\n",
      "Epoch 11, loss: 0.3111954927444458\n",
      "Epoch 12, loss: 0.3227260112762451\n",
      "Epoch 12, loss: 0.30657339096069336\n",
      "Epoch 12, loss: 0.29988738894462585\n",
      "Epoch 12, loss: 0.2630062997341156\n",
      "Epoch 12, loss: 0.32935136556625366\n",
      "Epoch 12, loss: 0.3637554347515106\n",
      "Epoch 12, loss: 0.3081958591938019\n",
      "Epoch 12, loss: 0.34904834628105164\n",
      "Epoch 12, loss: 0.2768576741218567\n",
      "Epoch 12, loss: 0.2933303415775299\n",
      "Epoch 12, loss: 0.37095242738723755\n",
      "Epoch 12, loss: 0.3485516309738159\n",
      "Epoch 13, loss: 0.30902257561683655\n",
      "Epoch 13, loss: 0.2858147919178009\n",
      "Epoch 13, loss: 0.25431153178215027\n",
      "Epoch 13, loss: 0.3238556385040283\n",
      "Epoch 13, loss: 0.3695579171180725\n",
      "Epoch 13, loss: 0.256994366645813\n",
      "Epoch 13, loss: 0.324847549200058\n",
      "Epoch 13, loss: 0.3404942452907562\n",
      "Epoch 13, loss: 0.3447173237800598\n",
      "Epoch 13, loss: 0.23753686249256134\n",
      "Epoch 13, loss: 0.22949133813381195\n",
      "Epoch 13, loss: 0.5642080903053284\n",
      "Epoch 14, loss: 0.3244556486606598\n",
      "Epoch 14, loss: 0.3353259861469269\n",
      "Epoch 14, loss: 0.2624780833721161\n",
      "Epoch 14, loss: 0.2984117269515991\n",
      "Epoch 14, loss: 0.2594316601753235\n",
      "Epoch 14, loss: 0.28951799869537354\n",
      "Epoch 14, loss: 0.32114818692207336\n",
      "Epoch 14, loss: 0.32221686840057373\n",
      "Epoch 14, loss: 0.3062443435192108\n",
      "Epoch 14, loss: 0.3020329475402832\n",
      "Epoch 14, loss: 0.3079456090927124\n",
      "Epoch 14, loss: 0.6060762405395508\n",
      "Epoch 15, loss: 0.21008196473121643\n",
      "Epoch 15, loss: 0.2816179692745209\n",
      "Epoch 15, loss: 0.2962512671947479\n",
      "Epoch 15, loss: 0.27612465620040894\n",
      "Epoch 15, loss: 0.26375505328178406\n",
      "Epoch 15, loss: 0.33869439363479614\n",
      "Epoch 15, loss: 0.2774997055530548\n",
      "Epoch 15, loss: 0.23671239614486694\n",
      "Epoch 15, loss: 0.26779627799987793\n",
      "Epoch 15, loss: 0.2708601951599121\n",
      "Epoch 15, loss: 0.3004409670829773\n",
      "Epoch 15, loss: 0.31330716609954834\n",
      "Epoch 16, loss: 0.28291061520576477\n",
      "Epoch 16, loss: 0.27078014612197876\n",
      "Epoch 16, loss: 0.311126172542572\n",
      "Epoch 16, loss: 0.2813509404659271\n",
      "Epoch 16, loss: 0.25045979022979736\n",
      "Epoch 16, loss: 0.3309671878814697\n",
      "Epoch 16, loss: 0.3311435282230377\n",
      "Epoch 16, loss: 0.2634734511375427\n",
      "Epoch 16, loss: 0.291079044342041\n",
      "Epoch 16, loss: 0.26453858613967896\n",
      "Epoch 16, loss: 0.3097969591617584\n",
      "Epoch 16, loss: 0.6050074100494385\n",
      "Epoch 17, loss: 0.264220654964447\n",
      "Epoch 17, loss: 0.2958323359489441\n",
      "Epoch 17, loss: 0.24566394090652466\n",
      "Epoch 17, loss: 0.270195871591568\n",
      "Epoch 17, loss: 0.21811677515506744\n",
      "Epoch 17, loss: 0.25976186990737915\n",
      "Epoch 17, loss: 0.23036831617355347\n",
      "Epoch 17, loss: 0.3272849917411804\n",
      "Epoch 17, loss: 0.25846874713897705\n",
      "Epoch 17, loss: 0.26083794236183167\n",
      "Epoch 17, loss: 0.3050330579280853\n",
      "Epoch 17, loss: 0.3980437219142914\n",
      "Epoch 18, loss: 0.2653960585594177\n",
      "Epoch 18, loss: 0.2662161886692047\n",
      "Epoch 18, loss: 0.2520216703414917\n",
      "Epoch 18, loss: 0.22134000062942505\n",
      "Epoch 18, loss: 0.25202202796936035\n",
      "Epoch 18, loss: 0.26441437005996704\n",
      "Epoch 18, loss: 0.2597388029098511\n",
      "Epoch 18, loss: 0.25797125697135925\n",
      "Epoch 18, loss: 0.2844087481498718\n",
      "Epoch 18, loss: 0.24591881036758423\n",
      "Epoch 18, loss: 0.3061254918575287\n",
      "Epoch 18, loss: 0.31069955229759216\n",
      "Epoch 19, loss: 0.2753020226955414\n",
      "Epoch 19, loss: 0.2758362293243408\n",
      "Epoch 19, loss: 0.19817480444908142\n",
      "Epoch 19, loss: 0.21531201899051666\n",
      "Epoch 19, loss: 0.26340699195861816\n",
      "Epoch 19, loss: 0.2787932753562927\n",
      "Epoch 19, loss: 0.234714537858963\n",
      "Epoch 19, loss: 0.24993130564689636\n",
      "Epoch 19, loss: 0.22530104219913483\n",
      "Epoch 19, loss: 0.2864345908164978\n",
      "Epoch 19, loss: 0.2261677235364914\n",
      "Epoch 19, loss: 0.3820040225982666\n",
      "Epoch 20, loss: 0.22946365177631378\n",
      "Epoch 20, loss: 0.2166832834482193\n",
      "Epoch 20, loss: 0.25979477167129517\n",
      "Epoch 20, loss: 0.2333572953939438\n",
      "Epoch 20, loss: 0.23617368936538696\n",
      "Epoch 20, loss: 0.27155470848083496\n",
      "Epoch 20, loss: 0.24567264318466187\n",
      "Epoch 20, loss: 0.24863296747207642\n",
      "Epoch 20, loss: 0.280810683965683\n",
      "Epoch 20, loss: 0.29147037863731384\n",
      "Epoch 20, loss: 0.2630833685398102\n",
      "Epoch 20, loss: 0.245557501912117\n",
      "Epoch 21, loss: 0.23085802793502808\n",
      "Epoch 21, loss: 0.24908602237701416\n",
      "Epoch 21, loss: 0.2164992243051529\n",
      "Epoch 21, loss: 0.20046471059322357\n",
      "Epoch 21, loss: 0.2798994183540344\n",
      "Epoch 21, loss: 0.20055778324604034\n",
      "Epoch 21, loss: 0.24077685177326202\n",
      "Epoch 21, loss: 0.2913486063480377\n",
      "Epoch 21, loss: 0.205631285905838\n",
      "Epoch 21, loss: 0.25855526328086853\n",
      "Epoch 21, loss: 0.2613753080368042\n",
      "Epoch 21, loss: 0.17707784473896027\n",
      "Epoch 22, loss: 0.2029626965522766\n",
      "Epoch 22, loss: 0.2019268125295639\n",
      "Epoch 22, loss: 0.2874031960964203\n",
      "Epoch 22, loss: 0.23665578663349152\n",
      "Epoch 22, loss: 0.2062963992357254\n",
      "Epoch 22, loss: 0.23180347681045532\n",
      "Epoch 22, loss: 0.22344715893268585\n",
      "Epoch 22, loss: 0.19618164002895355\n",
      "Epoch 22, loss: 0.25837934017181396\n",
      "Epoch 22, loss: 0.2316383570432663\n",
      "Epoch 22, loss: 0.21574385464191437\n",
      "Epoch 22, loss: 0.12403548508882523\n",
      "Epoch 23, loss: 0.2091863602399826\n",
      "Epoch 23, loss: 0.19654855132102966\n",
      "Epoch 23, loss: 0.17379474639892578\n",
      "Epoch 23, loss: 0.27798473834991455\n",
      "Epoch 23, loss: 0.23878179490566254\n",
      "Epoch 23, loss: 0.23377853631973267\n",
      "Epoch 23, loss: 0.16398707032203674\n",
      "Epoch 23, loss: 0.26004907488822937\n",
      "Epoch 23, loss: 0.20053139328956604\n",
      "Epoch 23, loss: 0.2426367849111557\n",
      "Epoch 23, loss: 0.22427812218666077\n",
      "Epoch 23, loss: 0.21879655122756958\n",
      "Epoch 24, loss: 0.2167767733335495\n",
      "Epoch 24, loss: 0.2150430828332901\n",
      "Epoch 24, loss: 0.20850688219070435\n",
      "Epoch 24, loss: 0.23950399458408356\n",
      "Epoch 24, loss: 0.21107275784015656\n",
      "Epoch 24, loss: 0.2550739347934723\n",
      "Epoch 24, loss: 0.19354458153247833\n",
      "Epoch 24, loss: 0.21783313155174255\n",
      "Epoch 24, loss: 0.21160921454429626\n",
      "Epoch 24, loss: 0.2381531000137329\n",
      "Epoch 24, loss: 0.21850688755512238\n",
      "Epoch 24, loss: 0.13948746025562286\n",
      "Epoch 25, loss: 0.22119958698749542\n",
      "Epoch 25, loss: 0.17707645893096924\n",
      "Epoch 25, loss: 0.225761279463768\n",
      "Epoch 25, loss: 0.22131651639938354\n",
      "Epoch 25, loss: 0.24126194417476654\n",
      "Epoch 25, loss: 0.21170270442962646\n",
      "Epoch 25, loss: 0.21707376837730408\n",
      "Epoch 25, loss: 0.20560413599014282\n",
      "Epoch 25, loss: 0.22193509340286255\n",
      "Epoch 25, loss: 0.2217051386833191\n",
      "Epoch 25, loss: 0.21230357885360718\n",
      "Epoch 25, loss: 0.19716782867908478\n",
      "Epoch 26, loss: 0.21208497881889343\n",
      "Epoch 26, loss: 0.21774747967720032\n",
      "Epoch 26, loss: 0.2196359783411026\n",
      "Epoch 26, loss: 0.18290191888809204\n",
      "Epoch 26, loss: 0.18673941493034363\n",
      "Epoch 26, loss: 0.21208275854587555\n",
      "Epoch 26, loss: 0.23721367120742798\n",
      "Epoch 26, loss: 0.20280882716178894\n",
      "Epoch 26, loss: 0.26284894347190857\n",
      "Epoch 26, loss: 0.19933660328388214\n",
      "Epoch 26, loss: 0.21016064286231995\n",
      "Epoch 26, loss: 0.23786523938179016\n",
      "Epoch 27, loss: 0.18869322538375854\n",
      "Epoch 27, loss: 0.1784687042236328\n",
      "Epoch 27, loss: 0.19238731265068054\n",
      "Epoch 27, loss: 0.18503904342651367\n",
      "Epoch 27, loss: 0.18677429854869843\n",
      "Epoch 27, loss: 0.22317390143871307\n",
      "Epoch 27, loss: 0.2059985101222992\n",
      "Epoch 27, loss: 0.2127024233341217\n",
      "Epoch 27, loss: 0.26978838443756104\n",
      "Epoch 27, loss: 0.18960590660572052\n",
      "Epoch 27, loss: 0.16198848187923431\n",
      "Epoch 27, loss: 0.15542219579219818\n",
      "Epoch 28, loss: 0.17243093252182007\n",
      "Epoch 28, loss: 0.20571894943714142\n",
      "Epoch 28, loss: 0.2219354510307312\n",
      "Epoch 28, loss: 0.2158391773700714\n",
      "Epoch 28, loss: 0.19443407654762268\n",
      "Epoch 28, loss: 0.22279095649719238\n",
      "Epoch 28, loss: 0.1701146960258484\n",
      "Epoch 28, loss: 0.17558085918426514\n",
      "Epoch 28, loss: 0.2253454625606537\n",
      "Epoch 28, loss: 0.23296932876110077\n",
      "Epoch 28, loss: 0.23173120617866516\n",
      "Epoch 28, loss: 0.2718380391597748\n",
      "Epoch 29, loss: 0.1848398745059967\n",
      "Epoch 29, loss: 0.1847924292087555\n",
      "Epoch 29, loss: 0.1813678741455078\n",
      "Epoch 29, loss: 0.20552270114421844\n",
      "Epoch 29, loss: 0.17330996692180634\n",
      "Epoch 29, loss: 0.18916772305965424\n",
      "Epoch 29, loss: 0.26485833525657654\n",
      "Epoch 29, loss: 0.1802903115749359\n",
      "Epoch 29, loss: 0.21392852067947388\n",
      "Epoch 29, loss: 0.21731187403202057\n",
      "Epoch 29, loss: 0.1807396411895752\n",
      "Epoch 29, loss: 0.44860029220581055\n",
      "Epoch 30, loss: 0.23145130276679993\n",
      "Epoch 30, loss: 0.17468562722206116\n",
      "Epoch 30, loss: 0.1697978973388672\n",
      "Epoch 30, loss: 0.19356445968151093\n",
      "Epoch 30, loss: 0.18163947761058807\n",
      "Epoch 30, loss: 0.2005160003900528\n",
      "Epoch 30, loss: 0.18304920196533203\n",
      "Epoch 30, loss: 0.2312300205230713\n",
      "Epoch 30, loss: 0.2280166894197464\n",
      "Epoch 30, loss: 0.1635168343782425\n",
      "Epoch 30, loss: 0.19234034419059753\n",
      "Epoch 30, loss: 0.4941137135028839\n",
      "Epoch 31, loss: 0.18683569133281708\n",
      "Epoch 31, loss: 0.20368163287639618\n",
      "Epoch 31, loss: 0.19777359068393707\n",
      "Epoch 31, loss: 0.17373676598072052\n",
      "Epoch 31, loss: 0.21807675063610077\n",
      "Epoch 31, loss: 0.16158397495746613\n",
      "Epoch 31, loss: 0.14339253306388855\n",
      "Epoch 31, loss: 0.2171061635017395\n",
      "Epoch 31, loss: 0.18040530383586884\n",
      "Epoch 31, loss: 0.1664087325334549\n",
      "Epoch 31, loss: 0.20902034640312195\n",
      "Epoch 31, loss: 0.19582624733448029\n",
      "Epoch 32, loss: 0.17734438180923462\n",
      "Epoch 32, loss: 0.1984298825263977\n",
      "Epoch 32, loss: 0.19318987429141998\n",
      "Epoch 32, loss: 0.18004609644412994\n",
      "Epoch 32, loss: 0.19097214937210083\n",
      "Epoch 32, loss: 0.16461117565631866\n",
      "Epoch 32, loss: 0.17199784517288208\n",
      "Epoch 32, loss: 0.16854208707809448\n",
      "Epoch 32, loss: 0.202976256608963\n",
      "Epoch 32, loss: 0.2286401242017746\n",
      "Epoch 32, loss: 0.2594563364982605\n",
      "Epoch 32, loss: 0.19001968204975128\n",
      "Epoch 33, loss: 0.205145925283432\n",
      "Epoch 33, loss: 0.15387564897537231\n",
      "Epoch 33, loss: 0.21362076699733734\n",
      "Epoch 33, loss: 0.18503716588020325\n",
      "Epoch 33, loss: 0.19779016077518463\n",
      "Epoch 33, loss: 0.169569730758667\n",
      "Epoch 33, loss: 0.19245412945747375\n",
      "Epoch 33, loss: 0.21999932825565338\n",
      "Epoch 33, loss: 0.2015097439289093\n",
      "Epoch 33, loss: 0.1964060515165329\n",
      "Epoch 33, loss: 0.1688874512910843\n",
      "Epoch 33, loss: 0.23117367923259735\n",
      "Epoch 34, loss: 0.18955808877944946\n",
      "Epoch 34, loss: 0.18394234776496887\n",
      "Epoch 34, loss: 0.17381598055362701\n",
      "Epoch 34, loss: 0.16041289269924164\n",
      "Epoch 34, loss: 0.1780623346567154\n",
      "Epoch 34, loss: 0.1750563234090805\n",
      "Epoch 34, loss: 0.18658322095870972\n",
      "Epoch 34, loss: 0.17560307681560516\n",
      "Epoch 34, loss: 0.17925210297107697\n",
      "Epoch 34, loss: 0.18891793489456177\n",
      "Epoch 34, loss: 0.20856034755706787\n",
      "Epoch 34, loss: 0.229465514421463\n",
      "Epoch 35, loss: 0.20317023992538452\n",
      "Epoch 35, loss: 0.14398223161697388\n",
      "Epoch 35, loss: 0.15176300704479218\n",
      "Epoch 35, loss: 0.16762076318264008\n",
      "Epoch 35, loss: 0.1601196676492691\n",
      "Epoch 35, loss: 0.1961921900510788\n",
      "Epoch 35, loss: 0.18825410306453705\n",
      "Epoch 35, loss: 0.1848214715719223\n",
      "Epoch 35, loss: 0.2104341685771942\n",
      "Epoch 35, loss: 0.21725420653820038\n",
      "Epoch 35, loss: 0.21068742871284485\n",
      "Epoch 35, loss: 0.32256805896759033\n",
      "Epoch 36, loss: 0.18187052011489868\n",
      "Epoch 36, loss: 0.17872080206871033\n",
      "Epoch 36, loss: 0.17367121577262878\n",
      "Epoch 36, loss: 0.2022836059331894\n",
      "Epoch 36, loss: 0.1833270639181137\n",
      "Epoch 36, loss: 0.19586528837680817\n",
      "Epoch 36, loss: 0.21682780981063843\n",
      "Epoch 36, loss: 0.18224157392978668\n",
      "Epoch 36, loss: 0.17822960019111633\n",
      "Epoch 36, loss: 0.18279841542243958\n",
      "Epoch 36, loss: 0.20587702095508575\n",
      "Epoch 36, loss: 0.11530953645706177\n",
      "Epoch 37, loss: 0.16821014881134033\n",
      "Epoch 37, loss: 0.17048956453800201\n",
      "Epoch 37, loss: 0.20485520362854004\n",
      "Epoch 37, loss: 0.1492612212896347\n",
      "Epoch 37, loss: 0.19766640663146973\n",
      "Epoch 37, loss: 0.18252280354499817\n",
      "Epoch 37, loss: 0.21027250587940216\n",
      "Epoch 37, loss: 0.16160143911838531\n",
      "Epoch 37, loss: 0.1591014266014099\n",
      "Epoch 37, loss: 0.18774603307247162\n",
      "Epoch 37, loss: 0.18654237687587738\n",
      "Epoch 37, loss: 0.2865574061870575\n",
      "Epoch 38, loss: 0.17217780649662018\n",
      "Epoch 38, loss: 0.20049607753753662\n",
      "Epoch 38, loss: 0.15048454701900482\n",
      "Epoch 38, loss: 0.16510292887687683\n",
      "Epoch 38, loss: 0.1955805867910385\n",
      "Epoch 38, loss: 0.19991172850131989\n",
      "Epoch 38, loss: 0.17643238604068756\n",
      "Epoch 38, loss: 0.16882726550102234\n",
      "Epoch 38, loss: 0.2265721559524536\n",
      "Epoch 38, loss: 0.19964072108268738\n",
      "Epoch 38, loss: 0.1687804013490677\n",
      "Epoch 38, loss: 0.23615367710590363\n",
      "Epoch 39, loss: 0.20261041820049286\n",
      "Epoch 39, loss: 0.14161567389965057\n",
      "Epoch 39, loss: 0.17714041471481323\n",
      "Epoch 39, loss: 0.17749769985675812\n",
      "Epoch 39, loss: 0.18839925527572632\n",
      "Epoch 39, loss: 0.2305067777633667\n",
      "Epoch 39, loss: 0.1543448567390442\n",
      "Epoch 39, loss: 0.16983157396316528\n",
      "Epoch 39, loss: 0.1630338579416275\n",
      "Epoch 39, loss: 0.1820419728755951\n",
      "Epoch 39, loss: 0.1879454404115677\n",
      "Epoch 39, loss: 0.24900808930397034\n",
      "Epoch 40, loss: 0.1564815640449524\n",
      "Epoch 40, loss: 0.20088231563568115\n",
      "Epoch 40, loss: 0.15072308480739594\n",
      "Epoch 40, loss: 0.21995829045772552\n",
      "Epoch 40, loss: 0.2148057222366333\n",
      "Epoch 40, loss: 0.1611250787973404\n",
      "Epoch 40, loss: 0.17613321542739868\n",
      "Epoch 40, loss: 0.21478261053562164\n",
      "Epoch 40, loss: 0.20287182927131653\n",
      "Epoch 40, loss: 0.1744181513786316\n",
      "Epoch 40, loss: 0.17292094230651855\n",
      "Epoch 40, loss: 0.16051577031612396\n",
      "Epoch 41, loss: 0.20040234923362732\n",
      "Epoch 41, loss: 0.21027587354183197\n",
      "Epoch 41, loss: 0.17245785892009735\n",
      "Epoch 41, loss: 0.2020735889673233\n",
      "Epoch 41, loss: 0.20797404646873474\n",
      "Epoch 41, loss: 0.16624270379543304\n",
      "Epoch 41, loss: 0.1501108705997467\n",
      "Epoch 41, loss: 0.22823865711688995\n",
      "Epoch 41, loss: 0.1656220257282257\n",
      "Epoch 41, loss: 0.21926073729991913\n",
      "Epoch 41, loss: 0.1659366637468338\n",
      "Epoch 41, loss: 0.28039324283599854\n",
      "Epoch 42, loss: 0.19790233671665192\n",
      "Epoch 42, loss: 0.21975241601467133\n",
      "Epoch 42, loss: 0.16742737591266632\n",
      "Epoch 42, loss: 0.21080899238586426\n",
      "Epoch 42, loss: 0.14987677335739136\n",
      "Epoch 42, loss: 0.18104490637779236\n",
      "Epoch 42, loss: 0.19421449303627014\n",
      "Epoch 42, loss: 0.1777074635028839\n",
      "Epoch 42, loss: 0.22653494775295258\n",
      "Epoch 42, loss: 0.19809676706790924\n",
      "Epoch 42, loss: 0.17146605253219604\n",
      "Epoch 42, loss: 0.25845688581466675\n",
      "Epoch 43, loss: 0.1839647740125656\n",
      "Epoch 43, loss: 0.21399261057376862\n",
      "Epoch 43, loss: 0.21193261444568634\n",
      "Epoch 43, loss: 0.15429732203483582\n",
      "Epoch 43, loss: 0.16675379872322083\n",
      "Epoch 43, loss: 0.1961347609758377\n",
      "Epoch 43, loss: 0.2593982517719269\n",
      "Epoch 43, loss: 0.15751536190509796\n",
      "Epoch 43, loss: 0.19769208133220673\n",
      "Epoch 43, loss: 0.16256487369537354\n",
      "Epoch 43, loss: 0.21404100954532623\n",
      "Epoch 43, loss: 0.4477793574333191\n",
      "Epoch 44, loss: 0.1719515174627304\n",
      "Epoch 44, loss: 0.16413690149784088\n",
      "Epoch 44, loss: 0.16676722466945648\n",
      "Epoch 44, loss: 0.22169806063175201\n",
      "Epoch 44, loss: 0.22613725066184998\n",
      "Epoch 44, loss: 0.1455308198928833\n",
      "Epoch 44, loss: 0.2001333385705948\n",
      "Epoch 44, loss: 0.19805105030536652\n",
      "Epoch 44, loss: 0.17210130393505096\n",
      "Epoch 44, loss: 0.18695031106472015\n",
      "Epoch 44, loss: 0.1783023476600647\n",
      "Epoch 44, loss: 0.22562669217586517\n",
      "Epoch 45, loss: 0.18470002710819244\n",
      "Epoch 45, loss: 0.2661218047142029\n",
      "Epoch 45, loss: 0.25055214762687683\n",
      "Epoch 45, loss: 0.19317349791526794\n",
      "Epoch 45, loss: 0.3145851194858551\n",
      "Epoch 45, loss: 0.20016692578792572\n",
      "Epoch 45, loss: 0.14434893429279327\n",
      "Epoch 45, loss: 0.18304720520973206\n",
      "Epoch 45, loss: 0.18806514143943787\n",
      "Epoch 45, loss: 0.19415372610092163\n",
      "Epoch 45, loss: 0.22476445138454437\n",
      "Epoch 45, loss: 0.2086142748594284\n",
      "Epoch 46, loss: 0.30230075120925903\n",
      "Epoch 46, loss: 0.21685592830181122\n",
      "Epoch 46, loss: 0.22335709631443024\n",
      "Epoch 46, loss: 0.19799119234085083\n",
      "Epoch 46, loss: 0.1802079975605011\n",
      "Epoch 46, loss: 0.2446979135274887\n",
      "Epoch 46, loss: 0.19546125829219818\n",
      "Epoch 46, loss: 0.17570199072360992\n",
      "Epoch 46, loss: 0.21001562476158142\n",
      "Epoch 46, loss: 0.2162644863128662\n",
      "Epoch 46, loss: 0.2432604432106018\n",
      "Epoch 46, loss: 0.1988193243741989\n",
      "Epoch 47, loss: 0.19955946505069733\n",
      "Epoch 47, loss: 0.21132510900497437\n",
      "Epoch 47, loss: 0.17312830686569214\n",
      "Epoch 47, loss: 0.19732409715652466\n",
      "Epoch 47, loss: 0.18893203139305115\n",
      "Epoch 47, loss: 0.2211708277463913\n",
      "Epoch 47, loss: 0.23675940930843353\n",
      "Epoch 47, loss: 0.19583670794963837\n",
      "Epoch 47, loss: 0.22820395231246948\n",
      "Epoch 47, loss: 0.2177211344242096\n",
      "Epoch 47, loss: 0.17657431960105896\n",
      "Epoch 47, loss: 0.14707651734352112\n",
      "Epoch 48, loss: 0.15952901542186737\n",
      "Epoch 48, loss: 0.19410990178585052\n",
      "Epoch 48, loss: 0.20818942785263062\n",
      "Epoch 48, loss: 0.14631244540214539\n",
      "Epoch 48, loss: 0.21608193218708038\n",
      "Epoch 48, loss: 0.18429769575595856\n",
      "Epoch 48, loss: 0.17827686667442322\n",
      "Epoch 48, loss: 0.19720081984996796\n",
      "Epoch 48, loss: 0.22318126261234283\n",
      "Epoch 48, loss: 0.16393667459487915\n",
      "Epoch 48, loss: 0.2051534652709961\n",
      "Epoch 48, loss: 0.177317276597023\n",
      "Epoch 49, loss: 0.19437949359416962\n",
      "Epoch 49, loss: 0.1993664652109146\n",
      "Epoch 49, loss: 0.26002663373947144\n",
      "Epoch 49, loss: 0.22780153155326843\n",
      "Epoch 49, loss: 0.21755194664001465\n",
      "Epoch 49, loss: 0.18288542330265045\n",
      "Epoch 49, loss: 0.1895252913236618\n",
      "Epoch 49, loss: 0.21222105622291565\n",
      "Epoch 49, loss: 0.18707415461540222\n",
      "Epoch 49, loss: 0.23484106361865997\n",
      "Epoch 49, loss: 0.19879676401615143\n",
      "Epoch 49, loss: 0.4152074456214905\n",
      "Epoch 50, loss: 0.20029044151306152\n",
      "Epoch 50, loss: 0.28643599152565\n",
      "Epoch 50, loss: 0.17581619322299957\n",
      "Epoch 50, loss: 0.2227167785167694\n",
      "Epoch 50, loss: 0.16371700167655945\n",
      "Epoch 50, loss: 0.18854112923145294\n",
      "Epoch 50, loss: 0.17982563376426697\n",
      "Epoch 50, loss: 0.16356851160526276\n",
      "Epoch 50, loss: 0.20224064588546753\n",
      "Epoch 50, loss: 0.23638971149921417\n",
      "Epoch 50, loss: 0.21316120028495789\n",
      "Epoch 50, loss: 0.21051160991191864\n",
      "Epoch 51, loss: 0.22034959495067596\n",
      "Epoch 51, loss: 0.17975398898124695\n",
      "Epoch 51, loss: 0.1893300861120224\n",
      "Epoch 51, loss: 0.17770953476428986\n",
      "Epoch 51, loss: 0.23800693452358246\n",
      "Epoch 51, loss: 0.20694111287593842\n",
      "Epoch 51, loss: 0.1719159632921219\n",
      "Epoch 51, loss: 0.17973752319812775\n",
      "Epoch 51, loss: 0.1550370305776596\n",
      "Epoch 51, loss: 0.1696261614561081\n",
      "Epoch 51, loss: 0.1683533489704132\n",
      "Epoch 51, loss: 0.19215095043182373\n",
      "Epoch 52, loss: 0.18945211172103882\n",
      "Epoch 52, loss: 0.1552913933992386\n",
      "Epoch 52, loss: 0.20142507553100586\n",
      "Epoch 52, loss: 0.19302575290203094\n",
      "Epoch 52, loss: 0.20018544793128967\n",
      "Epoch 52, loss: 0.19900035858154297\n",
      "Epoch 52, loss: 0.1580687016248703\n",
      "Epoch 52, loss: 0.12997041642665863\n",
      "Epoch 52, loss: 0.14464297890663147\n",
      "Epoch 52, loss: 0.19186553359031677\n",
      "Epoch 52, loss: 0.23296257853507996\n",
      "Epoch 52, loss: 0.19346320629119873\n",
      "Epoch 53, loss: 0.18337275087833405\n",
      "Epoch 53, loss: 0.15976814925670624\n",
      "Epoch 53, loss: 0.18279671669006348\n",
      "Epoch 53, loss: 0.14203393459320068\n",
      "Epoch 53, loss: 0.21764308214187622\n",
      "Epoch 53, loss: 0.18667326867580414\n",
      "Epoch 53, loss: 0.19073081016540527\n",
      "Epoch 53, loss: 0.18128295242786407\n",
      "Epoch 53, loss: 0.1621762365102768\n",
      "Epoch 53, loss: 0.2394895702600479\n",
      "Epoch 53, loss: 0.2006615251302719\n",
      "Epoch 53, loss: 0.18947362899780273\n",
      "Epoch 54, loss: 0.1735677570104599\n",
      "Epoch 54, loss: 0.16102370619773865\n",
      "Epoch 54, loss: 0.13063094019889832\n",
      "Epoch 54, loss: 0.15527218580245972\n",
      "Epoch 54, loss: 0.21644166111946106\n",
      "Epoch 54, loss: 0.22840170562267303\n",
      "Epoch 54, loss: 0.15297214686870575\n",
      "Epoch 54, loss: 0.20919884741306305\n",
      "Epoch 54, loss: 0.235178604722023\n",
      "Epoch 54, loss: 0.20987968146800995\n",
      "Epoch 54, loss: 0.13740116357803345\n",
      "Epoch 54, loss: 0.13447262346744537\n",
      "Epoch 55, loss: 0.19959652423858643\n",
      "Epoch 55, loss: 0.18878430128097534\n",
      "Epoch 55, loss: 0.2387959510087967\n",
      "Epoch 55, loss: 0.23900827765464783\n",
      "Epoch 55, loss: 0.15488369762897491\n",
      "Epoch 55, loss: 0.18493355810642242\n",
      "Epoch 55, loss: 0.23039992153644562\n",
      "Epoch 55, loss: 0.19247862696647644\n",
      "Epoch 55, loss: 0.22486090660095215\n",
      "Epoch 55, loss: 0.1823151558637619\n",
      "Epoch 55, loss: 0.18125489354133606\n",
      "Epoch 55, loss: 0.1669144183397293\n",
      "Epoch 56, loss: 0.19075660407543182\n",
      "Epoch 56, loss: 0.17039524018764496\n",
      "Epoch 56, loss: 0.18348608911037445\n",
      "Epoch 56, loss: 0.19567841291427612\n",
      "Epoch 56, loss: 0.196061909198761\n",
      "Epoch 56, loss: 0.14753443002700806\n",
      "Epoch 56, loss: 0.14679843187332153\n",
      "Epoch 56, loss: 0.2259717434644699\n",
      "Epoch 56, loss: 0.2886987030506134\n",
      "Epoch 56, loss: 0.17206834256649017\n",
      "Epoch 56, loss: 0.24551303684711456\n",
      "Epoch 56, loss: 0.1993110179901123\n",
      "Epoch 57, loss: 0.2027783989906311\n",
      "Epoch 57, loss: 0.21249417960643768\n",
      "Epoch 57, loss: 0.31952497363090515\n",
      "Epoch 57, loss: 0.17959816753864288\n",
      "Epoch 57, loss: 0.15579329431056976\n",
      "Epoch 57, loss: 0.2409866899251938\n",
      "Epoch 57, loss: 0.15026703476905823\n",
      "Epoch 57, loss: 0.22257106006145477\n",
      "Epoch 57, loss: 0.1625942587852478\n",
      "Epoch 57, loss: 0.15033233165740967\n",
      "Epoch 57, loss: 0.16586795449256897\n",
      "Epoch 57, loss: 0.22283394634723663\n",
      "Epoch 58, loss: 0.18546566367149353\n",
      "Epoch 58, loss: 0.1874210685491562\n",
      "Epoch 58, loss: 0.1535150110721588\n",
      "Epoch 58, loss: 0.15349476039409637\n",
      "Epoch 58, loss: 0.21772843599319458\n",
      "Epoch 58, loss: 0.1713850200176239\n",
      "Epoch 58, loss: 0.14006492495536804\n",
      "Epoch 58, loss: 0.16406017541885376\n",
      "Epoch 58, loss: 0.22293458878993988\n",
      "Epoch 58, loss: 0.1713545173406601\n",
      "Epoch 58, loss: 0.1856716275215149\n",
      "Epoch 58, loss: 0.6852781176567078\n",
      "Epoch 59, loss: 0.18859201669692993\n",
      "Epoch 59, loss: 0.17870056629180908\n",
      "Epoch 59, loss: 0.20514732599258423\n",
      "Epoch 59, loss: 0.21318133175373077\n",
      "Epoch 59, loss: 0.190132737159729\n",
      "Epoch 59, loss: 0.18528978526592255\n",
      "Epoch 59, loss: 0.2000240832567215\n",
      "Epoch 59, loss: 0.1782739907503128\n",
      "Epoch 59, loss: 0.15324148535728455\n",
      "Epoch 59, loss: 0.16311365365982056\n",
      "Epoch 59, loss: 0.21358390152454376\n",
      "Epoch 59, loss: 0.4429326355457306\n",
      "Epoch 60, loss: 0.2005419284105301\n",
      "Epoch 60, loss: 0.19042344391345978\n",
      "Epoch 60, loss: 0.19802691042423248\n",
      "Epoch 60, loss: 0.1569698452949524\n",
      "Epoch 60, loss: 0.17424707114696503\n",
      "Epoch 60, loss: 0.1799142211675644\n",
      "Epoch 60, loss: 0.16941888630390167\n",
      "Epoch 60, loss: 0.18165330588817596\n",
      "Epoch 60, loss: 0.20227575302124023\n",
      "Epoch 60, loss: 0.22009432315826416\n",
      "Epoch 60, loss: 0.2048519104719162\n",
      "Epoch 60, loss: 0.23228754103183746\n",
      "Epoch 61, loss: 0.17246496677398682\n",
      "Epoch 61, loss: 0.15286093950271606\n",
      "Epoch 61, loss: 0.12450706958770752\n",
      "Epoch 61, loss: 0.20555928349494934\n",
      "Epoch 61, loss: 0.21762850880622864\n",
      "Epoch 61, loss: 0.20375661551952362\n",
      "Epoch 61, loss: 0.18727204203605652\n",
      "Epoch 61, loss: 0.202093243598938\n",
      "Epoch 61, loss: 0.1606115996837616\n",
      "Epoch 61, loss: 0.1815352886915207\n",
      "Epoch 61, loss: 0.17631177604198456\n",
      "Epoch 61, loss: 0.20624573528766632\n",
      "Epoch 62, loss: 0.17917649447917938\n",
      "Epoch 62, loss: 0.1675688624382019\n",
      "Epoch 62, loss: 0.1913454234600067\n",
      "Epoch 62, loss: 0.21104063093662262\n",
      "Epoch 62, loss: 0.17691859602928162\n",
      "Epoch 62, loss: 0.21988968551158905\n",
      "Epoch 62, loss: 0.17295187711715698\n",
      "Epoch 62, loss: 0.21528762578964233\n",
      "Epoch 62, loss: 0.18600746989250183\n",
      "Epoch 62, loss: 0.19604234397411346\n",
      "Epoch 62, loss: 0.165290966629982\n",
      "Epoch 62, loss: 0.26017698645591736\n",
      "Epoch 63, loss: 0.18120509386062622\n",
      "Epoch 63, loss: 0.18937557935714722\n",
      "Epoch 63, loss: 0.20833441615104675\n",
      "Epoch 63, loss: 0.17315588891506195\n",
      "Epoch 63, loss: 0.18618836998939514\n",
      "Epoch 63, loss: 0.16565853357315063\n",
      "Epoch 63, loss: 0.17155694961547852\n",
      "Epoch 63, loss: 0.17942000925540924\n",
      "Epoch 63, loss: 0.1478857547044754\n",
      "Epoch 63, loss: 0.18760941922664642\n",
      "Epoch 63, loss: 0.209803968667984\n",
      "Epoch 63, loss: 0.23204796016216278\n",
      "Epoch 64, loss: 0.17091447114944458\n",
      "Epoch 64, loss: 0.18400539457798004\n",
      "Epoch 64, loss: 0.16929517686367035\n",
      "Epoch 64, loss: 0.18048948049545288\n",
      "Epoch 64, loss: 0.17944341897964478\n",
      "Epoch 64, loss: 0.18164008855819702\n",
      "Epoch 64, loss: 0.1615016758441925\n",
      "Epoch 64, loss: 0.15638193488121033\n",
      "Epoch 64, loss: 0.1709371954202652\n",
      "Epoch 64, loss: 0.15486018359661102\n",
      "Epoch 64, loss: 0.17663000524044037\n",
      "Epoch 64, loss: 0.22353284060955048\n",
      "Epoch 65, loss: 0.18832363188266754\n",
      "Epoch 65, loss: 0.19314272701740265\n",
      "Epoch 65, loss: 0.14961442351341248\n",
      "Epoch 65, loss: 0.1576087474822998\n",
      "Epoch 65, loss: 0.145864799618721\n",
      "Epoch 65, loss: 0.20945484936237335\n",
      "Epoch 65, loss: 0.16072221100330353\n",
      "Epoch 65, loss: 0.17302848398685455\n",
      "Epoch 65, loss: 0.1931878924369812\n",
      "Epoch 65, loss: 0.20878399908542633\n",
      "Epoch 65, loss: 0.17813284695148468\n",
      "Epoch 65, loss: 0.16332681477069855\n",
      "Epoch 66, loss: 0.1758134514093399\n",
      "Epoch 66, loss: 0.17845049500465393\n",
      "Epoch 66, loss: 0.1779354065656662\n",
      "Epoch 66, loss: 0.1693834811449051\n",
      "Epoch 66, loss: 0.21748259663581848\n",
      "Epoch 66, loss: 0.16512657701969147\n",
      "Epoch 66, loss: 0.1677756905555725\n",
      "Epoch 66, loss: 0.1766846925020218\n",
      "Epoch 66, loss: 0.18547050654888153\n",
      "Epoch 66, loss: 0.1715279370546341\n",
      "Epoch 66, loss: 0.16720719635486603\n",
      "Epoch 66, loss: 0.2245284765958786\n",
      "Epoch 67, loss: 0.17413358390331268\n",
      "Epoch 67, loss: 0.16964678466320038\n",
      "Epoch 67, loss: 0.19879329204559326\n",
      "Epoch 67, loss: 0.16957193613052368\n",
      "Epoch 67, loss: 0.1474023461341858\n",
      "Epoch 67, loss: 0.19957785308361053\n",
      "Epoch 67, loss: 0.1557493507862091\n",
      "Epoch 67, loss: 0.1812913417816162\n",
      "Epoch 67, loss: 0.26023948192596436\n",
      "Epoch 67, loss: 0.16873419284820557\n",
      "Epoch 67, loss: 0.18164853751659393\n",
      "Epoch 67, loss: 0.2122301608324051\n",
      "Epoch 68, loss: 0.17402306199073792\n",
      "Epoch 68, loss: 0.1761317253112793\n",
      "Epoch 68, loss: 0.18837152421474457\n",
      "Epoch 68, loss: 0.1679731160402298\n",
      "Epoch 68, loss: 0.19289222359657288\n",
      "Epoch 68, loss: 0.18847574293613434\n",
      "Epoch 68, loss: 0.20551177859306335\n",
      "Epoch 68, loss: 0.15425290167331696\n",
      "Epoch 68, loss: 0.15116195380687714\n",
      "Epoch 68, loss: 0.18748430907726288\n",
      "Epoch 68, loss: 0.1713898479938507\n",
      "Epoch 68, loss: 0.22584526240825653\n",
      "Epoch 69, loss: 0.18268640339374542\n",
      "Epoch 69, loss: 0.2666361629962921\n",
      "Epoch 69, loss: 0.13063198328018188\n",
      "Epoch 69, loss: 0.2013310045003891\n",
      "Epoch 69, loss: 0.17023828625679016\n",
      "Epoch 69, loss: 0.17489033937454224\n",
      "Epoch 69, loss: 0.1534697562456131\n",
      "Epoch 69, loss: 0.1685529202222824\n",
      "Epoch 69, loss: 0.21867595613002777\n",
      "Epoch 69, loss: 0.312631219625473\n",
      "Epoch 69, loss: 0.16011887788772583\n",
      "Epoch 69, loss: 0.09292740374803543\n",
      "Epoch 70, loss: 0.1537119746208191\n",
      "Epoch 70, loss: 0.15956923365592957\n",
      "Epoch 70, loss: 0.16953912377357483\n",
      "Epoch 70, loss: 0.19080130755901337\n",
      "Epoch 70, loss: 0.16835357248783112\n",
      "Epoch 70, loss: 0.17794638872146606\n",
      "Epoch 70, loss: 0.20392072200775146\n",
      "Epoch 70, loss: 0.208806112408638\n",
      "Epoch 70, loss: 0.18808241188526154\n",
      "Epoch 70, loss: 0.22574515640735626\n",
      "Epoch 70, loss: 0.16198547184467316\n",
      "Epoch 70, loss: 0.2346343994140625\n",
      "Epoch 71, loss: 0.16135172545909882\n",
      "Epoch 71, loss: 0.18650290369987488\n",
      "Epoch 71, loss: 0.13675838708877563\n",
      "Epoch 71, loss: 0.2274724841117859\n",
      "Epoch 71, loss: 0.13733786344528198\n",
      "Epoch 71, loss: 0.19074389338493347\n",
      "Epoch 71, loss: 0.16452611982822418\n",
      "Epoch 71, loss: 0.18221305310726166\n",
      "Epoch 71, loss: 0.20233744382858276\n",
      "Epoch 71, loss: 0.16644532978534698\n",
      "Epoch 71, loss: 0.18016690015792847\n",
      "Epoch 71, loss: 0.1535281389951706\n",
      "Epoch 72, loss: 0.15892598032951355\n",
      "Epoch 72, loss: 0.12968596816062927\n",
      "Epoch 72, loss: 0.17286594212055206\n",
      "Epoch 72, loss: 0.17234386503696442\n",
      "Epoch 72, loss: 0.1678614616394043\n",
      "Epoch 72, loss: 0.18020734190940857\n",
      "Epoch 72, loss: 0.16655299067497253\n",
      "Epoch 72, loss: 0.17473600804805756\n",
      "Epoch 72, loss: 0.2067764401435852\n",
      "Epoch 72, loss: 0.17340753972530365\n",
      "Epoch 72, loss: 0.21101629734039307\n",
      "Epoch 72, loss: 0.1796250343322754\n",
      "Epoch 73, loss: 0.16060903668403625\n",
      "Epoch 73, loss: 0.21035389602184296\n",
      "Epoch 73, loss: 0.1692647784948349\n",
      "Epoch 73, loss: 0.20607687532901764\n",
      "Epoch 73, loss: 0.18830440938472748\n",
      "Epoch 73, loss: 0.14171630144119263\n",
      "Epoch 73, loss: 0.16459384560585022\n",
      "Epoch 73, loss: 0.15952351689338684\n",
      "Epoch 73, loss: 0.15358783304691315\n",
      "Epoch 73, loss: 0.1992725431919098\n",
      "Epoch 73, loss: 0.1536986380815506\n",
      "Epoch 73, loss: 0.2211858034133911\n",
      "Epoch 74, loss: 0.2413419634103775\n",
      "Epoch 74, loss: 0.29636406898498535\n",
      "Epoch 74, loss: 0.16426090896129608\n",
      "Epoch 74, loss: 0.24065004289150238\n",
      "Epoch 74, loss: 0.16514427959918976\n",
      "Epoch 74, loss: 0.19656935334205627\n",
      "Epoch 74, loss: 0.16831903159618378\n",
      "Epoch 74, loss: 0.3344588577747345\n",
      "Epoch 74, loss: 0.20065435767173767\n",
      "Epoch 74, loss: 0.15653511881828308\n",
      "Epoch 74, loss: 0.21530736982822418\n",
      "Epoch 74, loss: 0.1835096925497055\n",
      "Epoch 75, loss: 0.2690891921520233\n",
      "Epoch 75, loss: 0.28883934020996094\n",
      "Epoch 75, loss: 0.22899208962917328\n",
      "Epoch 75, loss: 0.19983264803886414\n",
      "Epoch 75, loss: 0.19950605928897858\n",
      "Epoch 75, loss: 0.28409144282341003\n",
      "Epoch 75, loss: 0.18165531754493713\n",
      "Epoch 75, loss: 0.2654280364513397\n",
      "Epoch 75, loss: 0.14929772913455963\n",
      "Epoch 75, loss: 0.15744370222091675\n",
      "Epoch 75, loss: 0.20308494567871094\n",
      "Epoch 75, loss: 0.25035524368286133\n",
      "Epoch 76, loss: 0.17657095193862915\n",
      "Epoch 76, loss: 0.205447718501091\n",
      "Epoch 76, loss: 0.19584056735038757\n",
      "Epoch 76, loss: 0.2271479368209839\n",
      "Epoch 76, loss: 0.1898496299982071\n",
      "Epoch 76, loss: 0.20948834717273712\n",
      "Epoch 76, loss: 0.15177838504314423\n",
      "Epoch 76, loss: 0.17479878664016724\n",
      "Epoch 76, loss: 0.1937277466058731\n",
      "Epoch 76, loss: 0.17657527327537537\n",
      "Epoch 76, loss: 0.22020946443080902\n",
      "Epoch 76, loss: 0.21721267700195312\n",
      "Epoch 77, loss: 0.23427452147006989\n",
      "Epoch 77, loss: 0.16630595922470093\n",
      "Epoch 77, loss: 0.17324009537696838\n",
      "Epoch 77, loss: 0.21509777009487152\n",
      "Epoch 77, loss: 0.21028251945972443\n",
      "Epoch 77, loss: 0.196982741355896\n",
      "Epoch 77, loss: 0.16656191647052765\n",
      "Epoch 77, loss: 0.17462557554244995\n",
      "Epoch 77, loss: 0.19012939929962158\n",
      "Epoch 77, loss: 0.21534770727157593\n",
      "Epoch 77, loss: 0.18206004798412323\n",
      "Epoch 77, loss: 0.27171236276626587\n",
      "Epoch 78, loss: 0.19761121273040771\n",
      "Epoch 78, loss: 0.17649272084236145\n",
      "Epoch 78, loss: 0.15504971146583557\n",
      "Epoch 78, loss: 0.18629495799541473\n",
      "Epoch 78, loss: 0.15649931132793427\n",
      "Epoch 78, loss: 0.17563799023628235\n",
      "Epoch 78, loss: 0.24323131144046783\n",
      "Epoch 78, loss: 0.18057730793952942\n",
      "Epoch 78, loss: 0.19079774618148804\n",
      "Epoch 78, loss: 0.19505321979522705\n",
      "Epoch 78, loss: 0.23547498881816864\n",
      "Epoch 78, loss: 0.28191933035850525\n",
      "Epoch 79, loss: 0.16613633930683136\n",
      "Epoch 79, loss: 0.14480708539485931\n",
      "Epoch 79, loss: 0.16655333340168\n",
      "Epoch 79, loss: 0.1785707026720047\n",
      "Epoch 79, loss: 0.2620570659637451\n",
      "Epoch 79, loss: 0.19782422482967377\n",
      "Epoch 79, loss: 0.17715559899806976\n",
      "Epoch 79, loss: 0.1858169436454773\n",
      "Epoch 79, loss: 0.19159303605556488\n",
      "Epoch 79, loss: 0.1689879149198532\n",
      "Epoch 79, loss: 0.16749250888824463\n",
      "Epoch 79, loss: 0.27938392758369446\n",
      "Epoch 80, loss: 0.2148449569940567\n",
      "Epoch 80, loss: 0.18628118932247162\n",
      "Epoch 80, loss: 0.1760820895433426\n",
      "Epoch 80, loss: 0.17039670050144196\n",
      "Epoch 80, loss: 0.16177231073379517\n",
      "Epoch 80, loss: 0.1504085510969162\n",
      "Epoch 80, loss: 0.17717282474040985\n",
      "Epoch 80, loss: 0.1966429203748703\n",
      "Epoch 80, loss: 0.14608263969421387\n",
      "Epoch 80, loss: 0.266349732875824\n",
      "Epoch 80, loss: 0.1676534116268158\n",
      "Epoch 80, loss: 0.19730959832668304\n",
      "Epoch 81, loss: 0.15925082564353943\n",
      "Epoch 81, loss: 0.17630136013031006\n",
      "Epoch 81, loss: 0.215864360332489\n",
      "Epoch 81, loss: 0.17830118536949158\n",
      "Epoch 81, loss: 0.17201492190361023\n",
      "Epoch 81, loss: 0.17561069130897522\n",
      "Epoch 81, loss: 0.19647105038166046\n",
      "Epoch 81, loss: 0.15868987143039703\n",
      "Epoch 81, loss: 0.17518116533756256\n",
      "Epoch 81, loss: 0.17031680047512054\n",
      "Epoch 81, loss: 0.16927869617938995\n",
      "Epoch 81, loss: 0.23519372940063477\n",
      "Epoch 82, loss: 0.20804181694984436\n",
      "Epoch 82, loss: 0.16316933929920197\n",
      "Epoch 82, loss: 0.15655511617660522\n",
      "Epoch 82, loss: 0.16901260614395142\n",
      "Epoch 82, loss: 0.17780393362045288\n",
      "Epoch 82, loss: 0.171796977519989\n",
      "Epoch 82, loss: 0.17983116209506989\n",
      "Epoch 82, loss: 0.1425209641456604\n",
      "Epoch 82, loss: 0.17431998252868652\n",
      "Epoch 82, loss: 0.17875610291957855\n",
      "Epoch 82, loss: 0.1685587465763092\n",
      "Epoch 82, loss: 0.16039149463176727\n",
      "Epoch 83, loss: 0.13476121425628662\n",
      "Epoch 83, loss: 0.16105477511882782\n",
      "Epoch 83, loss: 0.17605659365653992\n",
      "Epoch 83, loss: 0.16760753095149994\n",
      "Epoch 83, loss: 0.15455840528011322\n",
      "Epoch 83, loss: 0.1412280648946762\n",
      "Epoch 83, loss: 0.1740577220916748\n",
      "Epoch 83, loss: 0.17474454641342163\n",
      "Epoch 83, loss: 0.18534475564956665\n",
      "Epoch 83, loss: 0.18656045198440552\n",
      "Epoch 83, loss: 0.22802585363388062\n",
      "Epoch 83, loss: 0.05118492245674133\n",
      "Epoch 84, loss: 0.18336449563503265\n",
      "Epoch 84, loss: 0.18281447887420654\n",
      "Epoch 84, loss: 0.18257230520248413\n",
      "Epoch 84, loss: 0.1888936161994934\n",
      "Epoch 84, loss: 0.1217537596821785\n",
      "Epoch 84, loss: 0.18081705272197723\n",
      "Epoch 84, loss: 0.16886115074157715\n",
      "Epoch 84, loss: 0.14566121995449066\n",
      "Epoch 84, loss: 0.21321900188922882\n",
      "Epoch 84, loss: 0.19521471858024597\n",
      "Epoch 84, loss: 0.1502598524093628\n",
      "Epoch 84, loss: 0.2551476061344147\n",
      "Epoch 85, loss: 0.19298839569091797\n",
      "Epoch 85, loss: 0.20302654802799225\n",
      "Epoch 85, loss: 0.18215346336364746\n",
      "Epoch 85, loss: 0.14723706245422363\n",
      "Epoch 85, loss: 0.1422497183084488\n",
      "Epoch 85, loss: 0.1583128124475479\n",
      "Epoch 85, loss: 0.17268848419189453\n",
      "Epoch 85, loss: 0.16801810264587402\n",
      "Epoch 85, loss: 0.1577758491039276\n",
      "Epoch 85, loss: 0.15575122833251953\n",
      "Epoch 85, loss: 0.2058802992105484\n",
      "Epoch 85, loss: 0.18196837604045868\n",
      "Epoch 86, loss: 0.17681634426116943\n",
      "Epoch 86, loss: 0.14755858480930328\n",
      "Epoch 86, loss: 0.18227161467075348\n",
      "Epoch 86, loss: 0.14310255646705627\n",
      "Epoch 86, loss: 0.1488122195005417\n",
      "Epoch 86, loss: 0.18736061453819275\n",
      "Epoch 86, loss: 0.15745340287685394\n",
      "Epoch 86, loss: 0.16369132697582245\n",
      "Epoch 86, loss: 0.17277705669403076\n",
      "Epoch 86, loss: 0.21084906160831451\n",
      "Epoch 86, loss: 0.2013169229030609\n",
      "Epoch 86, loss: 0.1536366194486618\n",
      "Epoch 87, loss: 0.16180458664894104\n",
      "Epoch 87, loss: 0.2045946717262268\n",
      "Epoch 87, loss: 0.14779628813266754\n",
      "Epoch 87, loss: 0.1597255915403366\n",
      "Epoch 87, loss: 0.153890460729599\n",
      "Epoch 87, loss: 0.19542177021503448\n",
      "Epoch 87, loss: 0.18714112043380737\n",
      "Epoch 87, loss: 0.19592216610908508\n",
      "Epoch 87, loss: 0.18871326744556427\n",
      "Epoch 87, loss: 0.15654350817203522\n",
      "Epoch 87, loss: 0.16920746862888336\n",
      "Epoch 87, loss: 0.06489511579275131\n",
      "Epoch 88, loss: 0.17247289419174194\n",
      "Epoch 88, loss: 0.1725599616765976\n",
      "Epoch 88, loss: 0.13496707379817963\n",
      "Epoch 88, loss: 0.15386517345905304\n",
      "Epoch 88, loss: 0.21226708590984344\n",
      "Epoch 88, loss: 0.1827544867992401\n",
      "Epoch 88, loss: 0.14810429513454437\n",
      "Epoch 88, loss: 0.1503494828939438\n",
      "Epoch 88, loss: 0.1794033944606781\n",
      "Epoch 88, loss: 0.17035207152366638\n",
      "Epoch 88, loss: 0.18156884610652924\n",
      "Epoch 88, loss: 0.20877596735954285\n",
      "Epoch 89, loss: 0.1888894885778427\n",
      "Epoch 89, loss: 0.15094362199306488\n",
      "Epoch 89, loss: 0.1765822470188141\n",
      "Epoch 89, loss: 0.16192695498466492\n",
      "Epoch 89, loss: 0.16570928692817688\n",
      "Epoch 89, loss: 0.16279909014701843\n",
      "Epoch 89, loss: 0.13096636533737183\n",
      "Epoch 89, loss: 0.16258707642555237\n",
      "Epoch 89, loss: 0.1791454702615738\n",
      "Epoch 89, loss: 0.18095816671848297\n",
      "Epoch 89, loss: 0.2108355462551117\n",
      "Epoch 89, loss: 0.1292020082473755\n",
      "Epoch 90, loss: 0.18243339657783508\n",
      "Epoch 90, loss: 0.18258972465991974\n",
      "Epoch 90, loss: 0.1360572874546051\n",
      "Epoch 90, loss: 0.1675141453742981\n",
      "Epoch 90, loss: 0.16398489475250244\n",
      "Epoch 90, loss: 0.22612354159355164\n",
      "Epoch 90, loss: 0.1483599841594696\n",
      "Epoch 90, loss: 0.1964520961046219\n",
      "Epoch 90, loss: 0.17289748787879944\n",
      "Epoch 90, loss: 0.18524639308452606\n",
      "Epoch 90, loss: 0.17420829832553864\n",
      "Epoch 90, loss: 0.18561497330665588\n",
      "Epoch 91, loss: 0.1730647087097168\n",
      "Epoch 91, loss: 0.16699886322021484\n",
      "Epoch 91, loss: 0.16424351930618286\n",
      "Epoch 91, loss: 0.1938903033733368\n",
      "Epoch 91, loss: 0.1578245759010315\n",
      "Epoch 91, loss: 0.141718327999115\n",
      "Epoch 91, loss: 0.17093998193740845\n",
      "Epoch 91, loss: 0.1790851652622223\n",
      "Epoch 91, loss: 0.2356468290090561\n",
      "Epoch 91, loss: 0.2150534689426422\n",
      "Epoch 91, loss: 0.16358689963817596\n",
      "Epoch 91, loss: 0.2415461540222168\n",
      "Epoch 92, loss: 0.15716327726840973\n",
      "Epoch 92, loss: 0.19723786413669586\n",
      "Epoch 92, loss: 0.13608860969543457\n",
      "Epoch 92, loss: 0.1967586725950241\n",
      "Epoch 92, loss: 0.2104925513267517\n",
      "Epoch 92, loss: 0.16273629665374756\n",
      "Epoch 92, loss: 0.2034544199705124\n",
      "Epoch 92, loss: 0.21156710386276245\n",
      "Epoch 92, loss: 0.1409710794687271\n",
      "Epoch 92, loss: 0.16419467329978943\n",
      "Epoch 92, loss: 0.20240522921085358\n",
      "Epoch 92, loss: 0.1858820766210556\n",
      "Epoch 93, loss: 0.1614648699760437\n",
      "Epoch 93, loss: 0.1525539755821228\n",
      "Epoch 93, loss: 0.18174505233764648\n",
      "Epoch 93, loss: 0.15831704437732697\n",
      "Epoch 93, loss: 0.1763809472322464\n",
      "Epoch 93, loss: 0.1672723889350891\n",
      "Epoch 93, loss: 0.15822955965995789\n",
      "Epoch 93, loss: 0.173674076795578\n",
      "Epoch 93, loss: 0.21061654388904572\n",
      "Epoch 93, loss: 0.20825515687465668\n",
      "Epoch 93, loss: 0.162131205201149\n",
      "Epoch 93, loss: 0.25047993659973145\n",
      "Epoch 94, loss: 0.20551832020282745\n",
      "Epoch 94, loss: 0.18631379306316376\n",
      "Epoch 94, loss: 0.16087760031223297\n",
      "Epoch 94, loss: 0.16564954817295074\n",
      "Epoch 94, loss: 0.20478594303131104\n",
      "Epoch 94, loss: 0.13601970672607422\n",
      "Epoch 94, loss: 0.18538036942481995\n",
      "Epoch 94, loss: 0.16381274163722992\n",
      "Epoch 94, loss: 0.1701800674200058\n",
      "Epoch 94, loss: 0.1806982010602951\n",
      "Epoch 94, loss: 0.1641032099723816\n",
      "Epoch 94, loss: 0.16519324481487274\n",
      "Epoch 95, loss: 0.17749068140983582\n",
      "Epoch 95, loss: 0.17405226826667786\n",
      "Epoch 95, loss: 0.14701378345489502\n",
      "Epoch 95, loss: 0.17237474024295807\n",
      "Epoch 95, loss: 0.15129807591438293\n",
      "Epoch 95, loss: 0.14484529197216034\n",
      "Epoch 95, loss: 0.1792396903038025\n",
      "Epoch 95, loss: 0.17644736170768738\n",
      "Epoch 95, loss: 0.21116188168525696\n",
      "Epoch 95, loss: 0.1467762440443039\n",
      "Epoch 95, loss: 0.21259517967700958\n",
      "Epoch 95, loss: 0.2854495048522949\n",
      "Epoch 96, loss: 0.19389255344867706\n",
      "Epoch 96, loss: 0.12765195965766907\n",
      "Epoch 96, loss: 0.14851568639278412\n",
      "Epoch 96, loss: 0.13244760036468506\n",
      "Epoch 96, loss: 0.17164137959480286\n",
      "Epoch 96, loss: 0.14256501197814941\n",
      "Epoch 96, loss: 0.17967548966407776\n",
      "Epoch 96, loss: 0.17660346627235413\n",
      "Epoch 96, loss: 0.23841848969459534\n",
      "Epoch 96, loss: 0.19774094223976135\n",
      "Epoch 96, loss: 0.21924234926700592\n",
      "Epoch 96, loss: 0.15322814881801605\n",
      "Epoch 97, loss: 0.1513020247220993\n",
      "Epoch 97, loss: 0.25631454586982727\n",
      "Epoch 97, loss: 0.15454411506652832\n",
      "Epoch 97, loss: 0.19015216827392578\n",
      "Epoch 97, loss: 0.25434547662734985\n",
      "Epoch 97, loss: 0.21138723194599152\n",
      "Epoch 97, loss: 0.14810001850128174\n",
      "Epoch 97, loss: 0.17514805495738983\n",
      "Epoch 97, loss: 0.21151626110076904\n",
      "Epoch 97, loss: 0.1794079691171646\n",
      "Epoch 97, loss: 0.17232581973075867\n",
      "Epoch 97, loss: 0.13381928205490112\n",
      "Epoch 98, loss: 0.2563186287879944\n",
      "Epoch 98, loss: 0.19602543115615845\n",
      "Epoch 98, loss: 0.23342493176460266\n",
      "Epoch 98, loss: 0.14600488543510437\n",
      "Epoch 98, loss: 0.21200193464756012\n",
      "Epoch 98, loss: 0.24871885776519775\n",
      "Epoch 98, loss: 0.1783735454082489\n",
      "Epoch 98, loss: 0.2866505980491638\n",
      "Epoch 98, loss: 0.20515622198581696\n",
      "Epoch 98, loss: 0.20661063492298126\n",
      "Epoch 98, loss: 0.15712778270244598\n",
      "Epoch 98, loss: 0.2531164884567261\n",
      "Epoch 99, loss: 0.18454600870609283\n",
      "Epoch 99, loss: 0.17854784429073334\n",
      "Epoch 99, loss: 0.24165157973766327\n",
      "Epoch 99, loss: 0.2351633757352829\n",
      "Epoch 99, loss: 0.192713662981987\n",
      "Epoch 99, loss: 0.1547642946243286\n",
      "Epoch 99, loss: 0.2322564274072647\n",
      "Epoch 99, loss: 0.1775883585214615\n",
      "Epoch 99, loss: 0.19082939624786377\n",
      "Epoch 99, loss: 0.17604148387908936\n",
      "Epoch 99, loss: 0.1767958104610443\n",
      "Epoch 99, loss: 0.20443487167358398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fe0728a3c74685a9edd22fd9267baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Dataset batch: 0/2==================\n",
      "Question: what is the blue shape?\n",
      "Prediction: : rectangle.\n",
      "Answer: rectangle\n",
      "Question: what color is the shape?\n",
      "Prediction: \n",
      "Answer: blue\n",
      "Question: does the image contain a rectangle?\n",
      "Prediction: \n",
      "Answer: yes\n",
      "Question: is there a triangle in the image?\n",
      "Prediction: \n",
      "Answer: no\n",
      "Question: is there a black shape?\n",
      "Prediction: \n",
      "Answer: no\n",
      "Question: does the image not contain a gray shape?\n",
      "Prediction:  yes..\n",
      "Answer: yes\n",
      "Question: is there a red shape in the image?\n",
      "Prediction:  no.\n",
      "Answer: no\n",
      "Question: does the image not contain a red shape?\n",
      "Prediction:  yes..\n",
      "Answer: yes\n",
      "Question: is there not a blue shape?\n",
      "Prediction: .:\n",
      "Answer: no\n",
      "accuracy: 0.0\n",
      "f1score: 0.0\n",
      "WUP Measure: 0.0 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_bertscore_f1: 0.49151268270280624 \n",
      "\n",
      "\n",
      "==================Dataset batch: 1/2==================\n",
      "Question: is there not a blue shape in the image?\n",
      "Prediction:  no.\n",
      "Answer: no\n",
      "Question: is there not a yellow shape?\n",
      "Prediction: .\n",
      "Answer: yes\n",
      "Question: is a teal shape present?\n",
      "Prediction: .\n",
      "Answer: no\n",
      "Question: what color is the triangle?\n",
      "Prediction: \n",
      "Answer: blue\n",
      "Question: what color is the shape?\n",
      "Prediction: \n",
      "Answer: blue\n",
      "Question: is there not a rectangle in the image?\n",
      "Prediction: \n",
      "Answer: yes\n",
      "Question: is there a red shape?\n",
      "Prediction: \n",
      "Answer: no\n",
      "Question: is there a green shape in the image?\n",
      "Prediction: \n",
      "Answer: no\n",
      "Question: is there not a teal shape?\n",
      "Prediction: \n",
      "Answer: yes\n",
      "accuracy: 0.0\n",
      "f1score: 0.0\n",
      "WUP Measure: 0.0 \n",
      "\n",
      "\n",
      "val_bertscore_f1: 0.29066164626015556 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Dataset batch: 2/2==================\n",
      "Question: what shape is in the image?\n",
      "Prediction:  triangle.: rectangle.\n",
      "Answer: triangle\n",
      "Question: what shape does the image contain?\n",
      "Prediction:  triangle.:\n",
      "Answer: triangle\n",
      "accuracy: 0.0\n",
      "f1score: 0.0\n",
      "WUP Measure: 0.0 \n",
      "\n",
      "\n",
      "val_bertscore_f1: 0.8563212454319 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6821e139470743df82929a7d92fbca4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Dataset batch: 0/2==================\n",
      "Question: what is the blue shape?\n",
      "Prediction: : rectangle.\n",
      "Answer: rectangle\n",
      "Question: what color is the shape?\n",
      "Prediction: \n",
      "Answer: blue\n",
      "Question: does the image contain a rectangle?\n",
      "Prediction: \n",
      "Answer: yes\n",
      "Question: is there a triangle in the image?\n",
      "Prediction: \n",
      "Answer: no\n",
      "Question: is there a black shape?\n",
      "Prediction: \n",
      "Answer: no\n",
      "Question: does the image not contain a gray shape?\n",
      "Prediction:  yes..\n",
      "Answer: yes\n",
      "Question: is there a red shape in the image?\n",
      "Prediction:  no.\n",
      "Answer: no\n",
      "Question: does the image not contain a red shape?\n",
      "Prediction:  yes..\n",
      "Answer: yes\n",
      "Question: is there not a blue shape?\n",
      "Prediction: .:\n",
      "Answer: no\n",
      "accuracy: 0.0\n",
      "f1score: 0.0\n",
      "WUP Measure: 0.0 \n",
      "\n",
      "\n",
      "val_bertscore_f1: 0.49151268270280624 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Dataset batch: 1/2==================\n",
      "Question: is there not a blue shape in the image?\n",
      "Prediction:  no.\n",
      "Answer: no\n",
      "Question: is there not a yellow shape?\n",
      "Prediction: .\n",
      "Answer: yes\n",
      "Question: is a teal shape present?\n",
      "Prediction: .\n",
      "Answer: no\n",
      "Question: what color is the triangle?\n",
      "Prediction: \n",
      "Answer: blue\n",
      "Question: what color is the shape?\n",
      "Prediction: \n",
      "Answer: blue\n",
      "Question: is there not a rectangle in the image?\n",
      "Prediction: \n",
      "Answer: yes\n",
      "Question: is there a red shape?\n",
      "Prediction: \n",
      "Answer: no\n",
      "Question: is there a green shape in the image?\n",
      "Prediction: \n",
      "Answer: no\n",
      "Question: is there not a teal shape?\n",
      "Prediction: \n",
      "Answer: yes\n",
      "accuracy: 0.0\n",
      "f1score: 0.0\n",
      "WUP Measure: 0.0 \n",
      "\n",
      "\n",
      "val_bertscore_f1: 0.29066164626015556 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Dataset batch: 2/2==================\n",
      "Question: what shape is in the image?\n",
      "Prediction:  triangle.: rectangle.\n",
      "Answer: triangle\n",
      "Question: what shape does the image contain?\n",
      "Prediction:  triangle.:\n",
      "Answer: triangle\n",
      "accuracy: 0.0\n",
      "f1score: 0.0\n",
      "WUP Measure: 0.0 \n",
      "\n",
      "\n",
      "val_bertscore_f1: 0.8563212454319 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          f1score          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val_bertscore_f1      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5461651682853699     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        wup_measure        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         f1score         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val_bertscore_f1     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5461651682853699    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       wup_measure       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "def train(module: L.LightningModule):\n",
    "    hyperparams = model_config['hyperparameters']\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"wup_measure\", patience=3, verbose=False, mode=\"min\")\n",
    "    print(hyperparameters.values())\n",
    "    trainer = L.Trainer(\n",
    "            accelerator=\"gpu\",\n",
    "            devices=[0],\n",
    "            max_epochs=100,\n",
    "            accumulate_grad_batches=hyperparams.get(\"accumulate_grad_batches\"),\n",
    "            check_val_every_n_epoch=100,\n",
    "            gradient_clip_val=hyperparams.get(\"gradient_clip_val\"),\n",
    "            precision=\"16-mixed\",\n",
    "            num_sanity_val_steps=0,\n",
    "            default_root_dir=model_config['local_checkpoint_dir'],\n",
    "            callbacks=[early_stop_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(module)\n",
    "    trainer.validate(module)\n",
    "\n",
    "train(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color is the shape?\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD26iiiszcKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArkvEHxH8P+GdVbTtRkuFuFRXIjhLDB6c11tfN/xn/5KFN/17RfyNe9kOX0cfi/Y1r2s3pptYxrTcI3R6f/AMLn8If89rz/AMBzR/wufwh/z2vP/Ac1830V9x/qjl/eX3/8A4/rUz6Q/wCFz+EP+e15/wCA5rubC9h1LT7e9tyTDcRrIhYYO0jI4r42r638If8AImaL/wBeUX/oAr5niLJcNl1KE6F7ybWrv0v2OihWlUbTNqiiivjzqCvm/wCM/wDyUKb/AK9ov5GvpCvE/ib4D8R+IPGMl/pmn+fbGCNA/movIHPBNfT8L16VHHOdWSiuV6t27HPiYtwsjxqiu0/4VP4z/wCgR/5Hj/8AiqP+FT+M/wDoEf8AkeP/AOKr9M/tTA/8/o/+BI8/2c+xxdfW/hD/AJEzRf8Aryi/9AFfPn/Cp/Gf/QI/8jx//FV9E+G7Saw8M6XaXKbJ4LWOORc5wwUAivjOLcXh69CkqM1JpvZp9DqwsZRbuj//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAABJElEQVR4Ae2aMQ6CQBBFAQk2Bqw00RjPY2Fv423svBUJHsXCSmy0kYglE2LzljEk325M5s/Oe9BsiF+3azTmXzLmw3/PrgX+bVAGZAAS0CMEAeJ2GcAIYYAMQIC4XQYwQhggAxAgbpcBjBAGyAAEiNtlACOEATIAAeJ2GcAIYYAMQIC4XQYwQhggAxAgbpcBjBAGyAAEiNtTnNAJKC/Tsso6fwUq5kVzPDyLvDF5oReostN5ZmYEKbeb9373KnIbpnfAEvGuZcCbuJ0nA5aIdy0D3sTtPBmwRLxrGfAmbufJgCXiXcuAN3E7TwYsEe9aBryJ23kyYIl414GvVdrbm/b+Y4gl1qt32nfYOOzX63Wd3B/xEAukk2i56Nkh8AJDHP135gflrB34+KyJkgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image\n",
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "example = train_dataset[4]\n",
    "image = example[0]\n",
    "\n",
    "text_inputs = processor.tokenizer(\n",
    "    example[1][\"question\"], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "question = example[1]['question']\n",
    "question = \"What color is the shape?\"\n",
    "text = f\"Question: {question} Answer:\"\n",
    "print(question)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          ...,\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281]],\n",
       "\n",
       "         [[1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          ...,\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598]],\n",
       "\n",
       "         [[1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          ...,\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900]]]],\n",
       "       device='cuda:0', dtype=torch.float16), 'input_ids': tensor([[    2, 45641,    35,   653,  3195,    16,     5,  3989,   116, 31652,\n",
       "            35]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to('cuda', torch.float16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " blue.: is\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# prepare image for the model\n",
    "import torch\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to('cuda', torch.float32).to('cuda', torch.float16)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "model.to('cuda')\n",
    "# generated_ids = model.generate(pixel_values, max_length=25)\n",
    "generated_ids = model.generate(**inputs)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
