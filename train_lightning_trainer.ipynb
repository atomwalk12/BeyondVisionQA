{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atomwalk12/Dropbox (Old)/notes/vision/project/BeyondVisionQA/model.py:128: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.metric = load_metric(\"bertscore\")\n",
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for bertscore contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/bertscore/bertscore.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ea83a8a2044d1380e946f16b55e3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from qadataset import QADataset\n",
    "from config import dataset_config, model_config\n",
    "\n",
    "MODEL_ID = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "\n",
    "train_dataset = QADataset(dataset_config, split=\"train[:6]\")\n",
    "val_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6b8fac3c5f4b42acc70890d72fb6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from trainer_blip2 import BLIP2ModelPLModule\n",
    "from trainer_blip2 import BLIP2PLModule\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from trainer_blip2 import BLIP2ModelPLModule\n",
    "from trainer_blip2 import BLIP2PLModule\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=\"all-linear\",\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "# processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(\"ybelkada/blip2-opt-2.7b-fp16-sharded\", device_map=\"auto\", load_in_8bit=True)\n",
    "# # Let's define the LoraConfig\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     target_modules=[\"q_proj\", \"k_proj\"]\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, collate_fn=BLIP2PLModule.train_textual_labels, batch_size=8, shuffle=True, num_workers=4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from config import dataset_config\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from config import metrics\n",
    "from transformers import AutoProcessor\n",
    "from dataset_configs.easy_vqa import translate\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "class BLIP2ModelPLModule(L.LightningModule):\n",
    "    def __init__(self, hyperparameters, model, train_dataset, val_dataset):\n",
    "        super().__init__()\n",
    "        self.hyperparams = hyperparameters\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "        self.batch_size = hyperparameters.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        inputs, labels = batch\n",
    "\n",
    "        outputs = self.model(**inputs,\n",
    "                            labels=labels)\n",
    "        loss = outputs.loss\n",
    "        print(f\"Epoch {self.current_epoch}, loss: {loss.item()}\")\n",
    "\n",
    "        self.log(\"train_loss\", loss, batch_size=self.batch_size)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "\n",
    "        inputs, answers = batch\n",
    "\n",
    "        # auto-regressively generate token IDs\n",
    "\n",
    "        \n",
    "        generated_ids = self.model.generate(**inputs)\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        print(f\"==================Dataset batch: {batch_idx}/{self.val_dataset.dataset_length // self.batch_size}==================\")\n",
    "        scores = []\n",
    "        i = 0\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            print(f\"Question: {self.val_dataset.dataset[batch_idx*self.batch_size+i]['question']}\")\n",
    "            print(f\"Prediction: {pred}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            i += 1\n",
    "\n",
    "        for metric in metrics:\n",
    "            scores = metric.compute(predictions=predictions, references=answers, model=self)\n",
    "            \n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "       return torch.optim.Adam(self.parameters(), lr=5e-4)\n",
    "        \n",
    "\n",
    "\n",
    "class BLIP2PLModule(BLIP2ModelPLModule):\n",
    "    \n",
    "    def __init__(self, config, model, train_dataset, val_dataset):\n",
    "        super().__init__(config, model, train_dataset, val_dataset)\n",
    "    \n",
    "    def train_numeric_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        batch_labels = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input, label = translate(ground_truth, training=True)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input)\n",
    "            batch_labels.append({ 'label_ids': label['label_ids'], 'scores': torch.from_numpy(label['scores'])})\n",
    "\n",
    "        # inputs = processor(images=images, text=texts, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
    "        inputs = processor(text=texts, images=images, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        result = []\n",
    "\n",
    "        for label in batch_labels:\n",
    "            scores = label['scores']\n",
    "            result.append(scores)\n",
    "        \n",
    "        return inputs, torch.stack(result)\n",
    "\n",
    "\n",
    "    \n",
    "    def train_textual_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input = translate(ground_truth, training=True)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input)    \n",
    "\n",
    "        # inputs = processor(images=images, text=texts, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
    "        inputs = processor(text=texts, images=images, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def eval_numeric_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        answers = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input, output = translate(ground_truth, training=False)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input) \n",
    "            answers.append(output)\n",
    "\n",
    "        inputs = processor(text=texts, images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        return inputs, answers\n",
    "    \n",
    "    def eval_textual_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        answers = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input, output = translate(ground_truth, training=False)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input) \n",
    "            answers.append(output)\n",
    "\n",
    "        inputs = processor(text=texts, images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        return inputs, answers\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        if model_config['classification']:\n",
    "            return DataLoader(self.train_dataset, collate_fn=BLIP2PLModule.train_numeric_labels, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "        else:\n",
    "            return DataLoader(self.train_dataset, collate_fn=BLIP2PLModule.train_textual_labels, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if model_config['classification']:\n",
    "            return DataLoader(self.val_dataset, collate_fn=BLIP2PLModule.eval_numeric_labels, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        else:\n",
    "            return DataLoader(self.val_dataset, collate_fn=BLIP2PLModule.eval_textual_labels, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "hyperparameters = model_config['hyperparameters']\n",
    "if model_config['target'] == 'blip2':\n",
    "    module = BLIP2PLModule(hyperparameters, model, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | PeftModel | 2.0 B  | train\n",
      "--------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "1.9 B     Non-trainable params\n",
      "2.0 B     Total params\n",
      "7,848.709 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([20, 1.67, 1, 1.0, 8, 0.0005, 9, 1337, 1, './result', True, [0.9, 0.999], 0.05, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b672d3af3044a53b3e4aa5de543b7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 5.429113864898682\n",
      "Epoch 1, loss: 3.272695541381836\n",
      "Epoch 2, loss: 1.666793704032898\n",
      "Epoch 3, loss: 1.1642820835113525\n",
      "Epoch 4, loss: 0.8134127855300903\n",
      "Epoch 5, loss: 0.6365694403648376\n",
      "Epoch 6, loss: 0.45635342597961426\n",
      "Epoch 7, loss: 0.32946255803108215\n",
      "Epoch 8, loss: 0.25055837631225586\n",
      "Epoch 9, loss: 0.1941279023885727\n",
      "Epoch 10, loss: 0.17371264100074768\n",
      "Epoch 11, loss: 0.14352408051490784\n",
      "Epoch 12, loss: 0.1439998894929886\n",
      "Epoch 13, loss: 0.13441799581050873\n",
      "Epoch 14, loss: 0.14155259728431702\n",
      "Epoch 15, loss: 0.27083176374435425\n",
      "Epoch 16, loss: 0.15360203385353088\n",
      "Epoch 17, loss: 0.14219903945922852\n",
      "Epoch 18, loss: 0.15540099143981934\n",
      "Epoch 19, loss: 0.11965132504701614\n",
      "Epoch 20, loss: 0.15960286557674408\n",
      "Epoch 21, loss: 0.15252457559108734\n",
      "Epoch 22, loss: 0.14977383613586426\n",
      "Epoch 23, loss: 0.15202933549880981\n",
      "Epoch 24, loss: 0.13918465375900269\n",
      "Epoch 25, loss: 0.15585893392562866\n",
      "Epoch 26, loss: 0.14625965058803558\n",
      "Epoch 27, loss: 0.1306527704000473\n",
      "Epoch 28, loss: 0.1422925889492035\n",
      "Epoch 29, loss: 0.1323726624250412\n",
      "Epoch 30, loss: 0.3223717510700226\n",
      "Epoch 31, loss: 0.1268964409828186\n",
      "Epoch 32, loss: 0.13272961974143982\n",
      "Epoch 33, loss: 0.17397786676883698\n",
      "Epoch 34, loss: 0.1301623284816742\n",
      "Epoch 35, loss: 0.14667464792728424\n",
      "Epoch 36, loss: 0.12156148254871368\n",
      "Epoch 37, loss: 0.12350740283727646\n",
      "Epoch 38, loss: 0.12055845558643341\n",
      "Epoch 39, loss: 0.1355677843093872\n",
      "Epoch 40, loss: 0.1262732893228531\n",
      "Epoch 41, loss: 0.10837956517934799\n",
      "Epoch 42, loss: 0.1278286725282669\n",
      "Epoch 43, loss: 0.13012447953224182\n",
      "Epoch 44, loss: 0.1424451321363449\n",
      "Epoch 45, loss: 0.13882708549499512\n",
      "Epoch 46, loss: 0.14471761882305145\n",
      "Epoch 47, loss: 0.12914516031742096\n",
      "Epoch 48, loss: 0.14868131279945374\n",
      "Epoch 49, loss: 1.3761374950408936\n",
      "Epoch 50, loss: 1.3368288278579712\n",
      "Epoch 51, loss: 0.16055534780025482\n",
      "Epoch 52, loss: 0.15900321304798126\n",
      "Epoch 53, loss: 0.13124702870845795\n",
      "Epoch 54, loss: 0.1341526359319687\n",
      "Epoch 55, loss: 0.13704757392406464\n",
      "Epoch 56, loss: 0.12687906622886658\n",
      "Epoch 57, loss: 0.22444447875022888\n",
      "Epoch 58, loss: 0.1397724449634552\n",
      "Epoch 59, loss: 0.13271549344062805\n",
      "Epoch 60, loss: 0.16835196316242218\n",
      "Epoch 61, loss: 0.13833394646644592\n",
      "Epoch 62, loss: 0.12486165016889572\n",
      "Epoch 63, loss: 0.12907084822654724\n",
      "Epoch 64, loss: 0.13896238803863525\n",
      "Epoch 65, loss: 0.13674788177013397\n",
      "Epoch 66, loss: 0.1404733508825302\n",
      "Epoch 67, loss: 0.11729233711957932\n",
      "Epoch 68, loss: 0.11391213536262512\n",
      "Epoch 69, loss: 0.27934229373931885\n",
      "Epoch 70, loss: 0.1379297822713852\n",
      "Epoch 71, loss: 0.12471102923154831\n",
      "Epoch 72, loss: 0.17516383528709412\n",
      "Epoch 73, loss: 0.12135531008243561\n",
      "Epoch 74, loss: 0.1191680058836937\n",
      "Epoch 75, loss: 0.13437245786190033\n",
      "Epoch 76, loss: 0.12068220973014832\n",
      "Epoch 77, loss: 0.11996521055698395\n",
      "Epoch 78, loss: 0.11819114536046982\n",
      "Epoch 79, loss: 0.13493426144123077\n",
      "Epoch 80, loss: 0.11239604651927948\n",
      "Epoch 81, loss: 0.1434241235256195\n",
      "Epoch 82, loss: 0.1296410858631134\n",
      "Epoch 83, loss: 0.12485036998987198\n",
      "Epoch 84, loss: 0.12665796279907227\n",
      "Epoch 85, loss: 0.11067792773246765\n",
      "Epoch 86, loss: 0.13543376326560974\n",
      "Epoch 87, loss: 0.1185089722275734\n",
      "Epoch 88, loss: 0.11849011480808258\n",
      "Epoch 89, loss: 0.1378728449344635\n",
      "Epoch 90, loss: 0.11632916331291199\n",
      "Epoch 91, loss: 0.12922343611717224\n",
      "Epoch 92, loss: 0.12686818838119507\n",
      "Epoch 93, loss: 0.12429289519786835\n",
      "Epoch 94, loss: 0.12824563682079315\n",
      "Epoch 95, loss: 0.12984058260917664\n",
      "Epoch 96, loss: 0.13121041655540466\n",
      "Epoch 97, loss: 0.1336616426706314\n",
      "Epoch 98, loss: 0.12160628288984299\n",
      "Epoch 99, loss: 0.13004890084266663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b809e28931049cca5d9b539192309e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Dataset batch: 0/0==================\n",
      "Question: what is the blue shape?\n",
      "Prediction: \n",
      "Answer: rectangle\n",
      "Question: what color is the shape?\n",
      "Prediction: \n",
      "Answer: blue\n",
      "Question: does the image contain a rectangle?\n",
      "Prediction: . yes\n",
      "Answer: yes\n",
      "Question: is there a triangle in the image?\n",
      "Prediction:  no.\n",
      "Answer: no\n",
      "Question: is there a black shape?\n",
      "Prediction: .\n",
      "Answer: no\n",
      "Question: does the image not contain a gray shape?\n",
      "Prediction:  yes.\n",
      "Answer: yes\n",
      "accuracy: 0.0\n",
      "f1score: 0.0\n",
      "WUP Measure: 0.0 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_bertscore_f1: 0.5876763860384623 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f2aa9f961a42bcafc590ffcb288839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Dataset batch: 0/0==================\n",
      "Question: what is the blue shape?\n",
      "Prediction: \n",
      "Answer: rectangle\n",
      "Question: what color is the shape?\n",
      "Prediction: \n",
      "Answer: blue\n",
      "Question: does the image contain a rectangle?\n",
      "Prediction: . yes\n",
      "Answer: yes\n",
      "Question: is there a triangle in the image?\n",
      "Prediction:  no.\n",
      "Answer: no\n",
      "Question: is there a black shape?\n",
      "Prediction: .\n",
      "Answer: no\n",
      "Question: does the image not contain a gray shape?\n",
      "Prediction:  yes.\n",
      "Answer: yes\n",
      "accuracy: 0.0\n",
      "f1score: 0.0\n",
      "WUP Measure: 0.0 \n",
      "\n",
      "\n",
      "val_bertscore_f1: 0.5876763860384623 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">      Validate metric      </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">          f1score          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">     val_bertscore_f1      </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    0.5876764059066772     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">        wup_measure        </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m         f1score         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m    val_bertscore_f1     \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   0.5876764059066772    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m       wup_measure       \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "def train(module: L.LightningModule):\n",
    "    hyperparams = model_config['hyperparameters']\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"wup_measure\", patience=3, verbose=False, mode=\"min\")\n",
    "    print(hyperparameters.values())\n",
    "    trainer = L.Trainer(\n",
    "            accelerator=\"gpu\",\n",
    "            devices=[0],\n",
    "            max_epochs=100,\n",
    "            accumulate_grad_batches=hyperparams.get(\"accumulate_grad_batches\"),\n",
    "            check_val_every_n_epoch=100,\n",
    "            gradient_clip_val=hyperparams.get(\"gradient_clip_val\"),\n",
    "            precision=\"16-mixed\",\n",
    "            num_sanity_val_steps=0,\n",
    "            default_root_dir=model_config['local_checkpoint_dir'],\n",
    "            callbacks=[early_stop_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(module)\n",
    "    trainer.validate(module)\n",
    "\n",
    "train(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What color is the shape?\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD26iiiszcKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArkvEHxH8P+GdVbTtRkuFuFRXIjhLDB6c11tfN/xn/5KFN/17RfyNe9kOX0cfi/Y1r2s3pptYxrTcI3R6f/AMLn8If89rz/AMBzR/wufwh/z2vP/Ac1830V9x/qjl/eX3/8A4/rUz6Q/wCFz+EP+e15/wCA5rubC9h1LT7e9tyTDcRrIhYYO0jI4r42r638If8AImaL/wBeUX/oAr5niLJcNl1KE6F7ybWrv0v2OihWlUbTNqiiivjzqCvm/wCM/wDyUKb/AK9ov5GvpCvE/ib4D8R+IPGMl/pmn+fbGCNA/movIHPBNfT8L16VHHOdWSiuV6t27HPiYtwsjxqiu0/4VP4z/wCgR/5Hj/8AiqP+FT+M/wDoEf8AkeP/AOKr9M/tTA/8/o/+BI8/2c+xxdfW/hD/AJEzRf8Aryi/9AFfPn/Cp/Gf/QI/8jx//FV9E+G7Saw8M6XaXKbJ4LWOORc5wwUAivjOLcXh69CkqM1JpvZp9DqwsZRbuj//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAABJElEQVR4Ae2aMQ6CQBBFAQk2Bqw00RjPY2Fv423svBUJHsXCSmy0kYglE2LzljEk325M5s/Oe9BsiF+3azTmXzLmw3/PrgX+bVAGZAAS0CMEAeJ2GcAIYYAMQIC4XQYwQhggAxAgbpcBjBAGyAAEiNtlACOEATIAAeJ2GcAIYYAMQIC4XQYwQhggAxAgbpcBjBAGyAAEiNtTnNAJKC/Tsso6fwUq5kVzPDyLvDF5oReostN5ZmYEKbeb9373KnIbpnfAEvGuZcCbuJ0nA5aIdy0D3sTtPBmwRLxrGfAmbufJgCXiXcuAN3E7TwYsEe9aBryJ23kyYIl414GvVdrbm/b+Y4gl1qt32nfYOOzX63Wd3B/xEAukk2i56Nkh8AJDHP135gflrB34+KyJkgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image\n",
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "example = train_dataset[3]\n",
    "image = example[0]\n",
    "\n",
    "text_inputs = processor.tokenizer(\n",
    "    example[1][\"question\"], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "question = example[1]['question']\n",
    "question = \"What color is the shape?\"\n",
    "text = f\"Question: {question} Answer:\"\n",
    "print(question)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          ...,\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281]],\n",
       "\n",
       "         [[1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          ...,\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598]],\n",
       "\n",
       "         [[1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          ...,\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900]]]],\n",
       "       device='cuda:0', dtype=torch.float16), 'input_ids': tensor([[    2, 45641,    35,   653,  3195,    16,     5,  3989,   116, 31652,\n",
       "            35]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to('cuda', torch.float16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " blue.\n"
     ]
    }
   ],
   "source": [
    "# prepare image for the model\n",
    "import torch\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to('cuda', torch.float32).to('cuda', torch.float16)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "model.to('cuda')\n",
    "# generated_ids = model.generate(pixel_values, max_length=25)\n",
    "generated_ids = model.generate(**inputs)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
