{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atomwalk12/Dropbox (Old)/notes/vision/project/BeyondVisionQA/model.py:128: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.metric = load_metric(\"bertscore\")\n",
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for bertscore contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/bertscore/bertscore.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0cb51bd0dc4b96974bb49e03a1a395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from qadataset import QADataset\n",
    "from config import dataset_config, model_config\n",
    "\n",
    "MODEL_ID = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "\n",
    "train_dataset = QADataset(dataset_config, split=\"train[:6]\")\n",
    "val_dataset = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26fee77ab354b0cad464fb7cddc2154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from trainer_blip2 import BLIP2ModelPLModule\n",
    "from trainer_blip2 import BLIP2PLModule\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=\"all-linear\",\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from config import dataset_config\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from config import metrics\n",
    "from transformers import AutoProcessor\n",
    "from dataset_configs.easy_vqa import translate\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right\n",
    "\n",
    "class BLIP2ModelPLModule(L.LightningModule):\n",
    "    def __init__(self, hyperparameters, model, train_dataset, val_dataset):\n",
    "        super().__init__()\n",
    "        self.hyperparams = hyperparameters\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "        self.batch_size = hyperparameters.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        inputs, labels = batch\n",
    "\n",
    "        outputs = self.model(**inputs,\n",
    "                            labels=labels)\n",
    "        loss = outputs.loss\n",
    "        print(f\"Epoch {self.current_epoch}, loss: {loss.item()}\")\n",
    "\n",
    "        self.log(\"train_loss\", loss, batch_size=self.batch_size)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "\n",
    "        inputs, answers = batch\n",
    "\n",
    "        # auto-regressively generate token IDs\n",
    "\n",
    "        \n",
    "        generated_ids = self.model.generate(**inputs)\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        print(f\"==================Dataset batch: {batch_idx}/{self.val_dataset.dataset_length // self.batch_size}==================\")\n",
    "        scores = []\n",
    "        i = 0\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            print(f\"Question: {self.val_dataset.dataset[batch_idx*self.batch_size+i]['question']}\")\n",
    "            print(f\"Prediction: {pred}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            i += 1\n",
    "\n",
    "        for metric in metrics:\n",
    "            scores = metric.compute(predictions=predictions, references=answers, model=self)\n",
    "            \n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "       return torch.optim.Adam(self.parameters(), lr=5e-4)\n",
    "        \n",
    "\n",
    "\n",
    "class BLIP2PLModule(BLIP2ModelPLModule):\n",
    "    \n",
    "    def __init__(self, config, model, train_dataset, val_dataset):\n",
    "        super().__init__(config, model, train_dataset, val_dataset)\n",
    "    \n",
    "    def train_numeric_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        batch_labels = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input, label = translate(ground_truth, training=True)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input)\n",
    "            batch_labels.append({ 'label_ids': label['label_ids'], 'scores': torch.from_numpy(label['scores'])})\n",
    "\n",
    "        # inputs = processor(images=images, text=texts, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
    "        inputs = processor(text=texts, images=images, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        result = []\n",
    "\n",
    "        for label in batch_labels:\n",
    "            scores = label['scores']\n",
    "            result.append(scores)\n",
    "        \n",
    "        return inputs, torch.stack(result)\n",
    "\n",
    "\n",
    "    \n",
    "    def train_textual_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input = translate(ground_truth, training=True)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input)    \n",
    "\n",
    "        # inputs = processor(images=images, text=texts, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
    "        inputs = processor(text=texts, images=images, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def eval_numeric_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        answers = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input, output = translate(ground_truth, training=False)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input) \n",
    "            answers.append(output)\n",
    "\n",
    "        inputs = processor(text=texts, images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        return inputs, answers\n",
    "    \n",
    "    def eval_textual_labels(examples):\n",
    "        images = []\n",
    "        texts = []\n",
    "        answers = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            input, output = translate(ground_truth, training=False)\n",
    "            \n",
    "            images.append(image)\n",
    "            texts.append(input) \n",
    "            answers.append(output)\n",
    "\n",
    "        inputs = processor(text=texts, images=images, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        return inputs, answers\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        if model_config['classification']:\n",
    "            return DataLoader(self.train_dataset, collate_fn=BLIP2PLModule.train_numeric_labels, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "        else:\n",
    "            return DataLoader(self.train_dataset, collate_fn=BLIP2PLModule.train_textual_labels, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if model_config['classification']:\n",
    "            return DataLoader(self.val_dataset, collate_fn=BLIP2PLModule.eval_numeric_labels, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        else:\n",
    "            return DataLoader(self.val_dataset, collate_fn=BLIP2PLModule.eval_textual_labels, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "hyperparameters = model_config['hyperparameters']\n",
    "if model_config['target'] == 'blip2':\n",
    "    module = BLIP2PLModule(hyperparameters, model, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\": 10,\n",
    "          # \"val_check_interval\": 0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 8,\n",
    "          \"lr\": 1e-4,\n",
    "          \"batch_size\": 2,\n",
    "          \"seed\":2022,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 50,\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": True,\n",
    "}\n",
    "\n",
    "model_module = BLIP2PLModule(config, model, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | PeftModel | 2.0 B  | train\n",
      "--------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "1.9 B     Non-trainable params\n",
      "2.0 B     Total params\n",
      "7,848.709 Total estimated model params size (MB)\n",
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82e843340e34027a0b69e5f999c5019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1.3750027418136597\n",
      "Epoch 0, loss: 0.637110710144043\n",
      "Epoch 0, loss: 1.4908027648925781\n",
      "Epoch 1, loss: 1.6925950050354004\n",
      "Epoch 1, loss: 0.6320883631706238\n",
      "Epoch 1, loss: 1.5352731943130493\n",
      "Epoch 2, loss: 1.4059560298919678\n",
      "Epoch 2, loss: 2.0789883136749268\n",
      "Epoch 2, loss: 2.6406030654907227\n",
      "Epoch 3, loss: 1.2481745481491089\n",
      "Epoch 3, loss: 1.6230548620224\n",
      "Epoch 3, loss: 2.206968069076538\n",
      "Epoch 4, loss: 1.205672025680542\n",
      "Epoch 4, loss: 0.893151581287384\n",
      "Epoch 4, loss: 0.8688778877258301\n",
      "Epoch 5, loss: 0.4330177307128906\n",
      "Epoch 5, loss: 0.6430100798606873\n",
      "Epoch 5, loss: 0.5848237872123718\n",
      "Epoch 6, loss: 0.596420407295227\n",
      "Epoch 6, loss: 0.5761755108833313\n",
      "Epoch 6, loss: 0.5866084694862366\n",
      "Epoch 7, loss: 0.6213467717170715\n",
      "Epoch 7, loss: 0.18414480984210968\n",
      "Epoch 7, loss: 0.6228087544441223\n",
      "Epoch 8, loss: 0.2063680738210678\n",
      "Epoch 8, loss: 0.39960286021232605\n",
      "Epoch 8, loss: 0.9112968444824219\n",
      "Epoch 9, loss: 0.3898521363735199\n",
      "Epoch 9, loss: 0.34126925468444824\n",
      "Epoch 9, loss: 0.36739179491996765\n",
      "Epoch 10, loss: 0.30892032384872437\n",
      "Epoch 10, loss: 0.33591243624687195\n",
      "Epoch 10, loss: 0.2609747648239136\n",
      "Epoch 11, loss: 0.21338285505771637\n",
      "Epoch 11, loss: 0.2686496376991272\n",
      "Epoch 11, loss: 0.2700931131839752\n",
      "Epoch 12, loss: 0.20028181374073029\n",
      "Epoch 12, loss: 0.18523722887039185\n",
      "Epoch 12, loss: 0.2414344996213913\n",
      "Epoch 13, loss: 0.193688303232193\n",
      "Epoch 13, loss: 0.14875036478042603\n",
      "Epoch 13, loss: 0.19782955944538116\n",
      "Epoch 14, loss: 0.15410679578781128\n",
      "Epoch 14, loss: 0.1767846941947937\n",
      "Epoch 14, loss: 0.16844579577445984\n",
      "Epoch 15, loss: 0.17227183282375336\n",
      "Epoch 15, loss: 0.13231025636196136\n",
      "Epoch 15, loss: 0.12510918080806732\n",
      "Epoch 16, loss: 0.15518738329410553\n",
      "Epoch 16, loss: 0.1077924445271492\n",
      "Epoch 16, loss: 0.16404147446155548\n",
      "Epoch 17, loss: 0.16948862373828888\n",
      "Epoch 17, loss: 0.14939798414707184\n",
      "Epoch 17, loss: 0.11605606228113174\n",
      "Epoch 18, loss: 0.12187733501195908\n",
      "Epoch 18, loss: 0.11569781601428986\n",
      "Epoch 18, loss: 0.15055085718631744\n",
      "Epoch 19, loss: 0.18350346386432648\n",
      "Epoch 19, loss: 0.1345672756433487\n",
      "Epoch 19, loss: 0.12874868512153625\n",
      "Epoch 20, loss: 0.14288724958896637\n",
      "Epoch 20, loss: 0.12520895898342133\n",
      "Epoch 20, loss: 0.1552620679140091\n",
      "Epoch 21, loss: 0.11861106008291245\n",
      "Epoch 21, loss: 0.13056644797325134\n",
      "Epoch 21, loss: 0.15752121806144714\n",
      "Epoch 22, loss: 0.08939970284700394\n",
      "Epoch 22, loss: 0.1684325784444809\n",
      "Epoch 22, loss: 0.21245378255844116\n",
      "Epoch 23, loss: 0.1937168389558792\n",
      "Epoch 23, loss: 0.15514744818210602\n",
      "Epoch 23, loss: 0.1023363545536995\n",
      "Epoch 24, loss: 0.12620888650417328\n",
      "Epoch 24, loss: 0.16034744679927826\n",
      "Epoch 24, loss: 0.13812538981437683\n",
      "Epoch 25, loss: 0.1517922431230545\n",
      "Epoch 25, loss: 0.1210431233048439\n",
      "Epoch 25, loss: 0.1344020515680313\n",
      "Epoch 26, loss: 0.13909432291984558\n",
      "Epoch 26, loss: 0.1582208275794983\n",
      "Epoch 26, loss: 0.1284850388765335\n",
      "Epoch 27, loss: 0.12309221923351288\n",
      "Epoch 27, loss: 0.1364119052886963\n",
      "Epoch 27, loss: 0.20874831080436707\n",
      "Epoch 28, loss: 0.1329851597547531\n",
      "Epoch 28, loss: 0.20686595141887665\n",
      "Epoch 28, loss: 0.12418080121278763\n",
      "Epoch 29, loss: 0.12231630086898804\n",
      "Epoch 29, loss: 0.1161976009607315\n",
      "Epoch 29, loss: 0.16391457617282867\n",
      "Epoch 30, loss: 0.15276911854743958\n",
      "Epoch 30, loss: 0.15413139760494232\n",
      "Epoch 30, loss: 0.12421952933073044\n",
      "Epoch 31, loss: 0.10913386195898056\n",
      "Epoch 31, loss: 0.14183743298053741\n",
      "Epoch 31, loss: 0.1522284597158432\n",
      "Epoch 32, loss: 0.137686088681221\n",
      "Epoch 32, loss: 0.13835759460926056\n",
      "Epoch 32, loss: 0.11538360267877579\n",
      "Epoch 33, loss: 0.3318885564804077\n",
      "Epoch 33, loss: 0.14443887770175934\n",
      "Epoch 33, loss: 0.1526287943124771\n",
      "Epoch 34, loss: 2.183289051055908\n",
      "Epoch 34, loss: 2.050506353378296\n",
      "Epoch 34, loss: 2.4251296520233154\n",
      "Epoch 35, loss: 1.6745238304138184\n",
      "Epoch 35, loss: 2.0698137283325195\n",
      "Epoch 35, loss: 2.552018404006958\n",
      "Epoch 36, loss: 0.21189457178115845\n",
      "Epoch 36, loss: 0.46603748202323914\n",
      "Epoch 36, loss: 0.3320709764957428\n",
      "Epoch 37, loss: 0.23661372065544128\n",
      "Epoch 37, loss: 0.09282048046588898\n",
      "Epoch 37, loss: 0.2345060557126999\n",
      "Epoch 38, loss: 0.1681879758834839\n",
      "Epoch 38, loss: 0.13586030900478363\n",
      "Epoch 38, loss: 0.13221995532512665\n",
      "Epoch 39, loss: 0.21056704223155975\n",
      "Epoch 39, loss: 0.15565060079097748\n",
      "Epoch 39, loss: 0.13688312470912933\n",
      "Epoch 40, loss: 0.11530018597841263\n",
      "Epoch 40, loss: 0.194102942943573\n",
      "Epoch 40, loss: 0.14431609213352203\n",
      "Epoch 41, loss: 0.16163916885852814\n",
      "Epoch 41, loss: 0.14376579225063324\n",
      "Epoch 41, loss: 0.14879584312438965\n",
      "Epoch 42, loss: 0.14706845581531525\n",
      "Epoch 42, loss: 0.1575043648481369\n",
      "Epoch 42, loss: 0.12869063019752502\n",
      "Epoch 43, loss: 0.1447979360818863\n",
      "Epoch 43, loss: 0.1174226775765419\n",
      "Epoch 43, loss: 0.1742611974477768\n",
      "Epoch 44, loss: 0.15530909597873688\n",
      "Epoch 44, loss: 0.15853172540664673\n",
      "Epoch 44, loss: 0.15295273065567017\n",
      "Epoch 45, loss: 0.13626590371131897\n",
      "Epoch 45, loss: 0.1586090624332428\n",
      "Epoch 45, loss: 0.15789097547531128\n",
      "Epoch 46, loss: 0.11805784702301025\n",
      "Epoch 46, loss: 0.23810012638568878\n",
      "Epoch 46, loss: 0.18677571415901184\n",
      "Epoch 47, loss: 0.14633969962596893\n",
      "Epoch 47, loss: 0.12515917420387268\n",
      "Epoch 47, loss: 0.1517745703458786\n",
      "Epoch 48, loss: 0.12630882859230042\n",
      "Epoch 48, loss: 0.2112681120634079\n",
      "Epoch 48, loss: 0.13077668845653534\n",
      "Epoch 49, loss: 0.1965944468975067\n",
      "Epoch 49, loss: 0.11220945417881012\n",
      "Epoch 49, loss: 0.11042259633541107\n",
      "Epoch 50, loss: 0.1830810010433197\n",
      "Epoch 50, loss: 0.12709970772266388\n",
      "Epoch 50, loss: 0.11600121110677719\n",
      "Epoch 51, loss: 0.11227776855230331\n",
      "Epoch 51, loss: 0.16691967844963074\n",
      "Epoch 51, loss: 0.14566819369792938\n",
      "Epoch 52, loss: 0.10874556005001068\n",
      "Epoch 52, loss: 0.12507949769496918\n",
      "Epoch 52, loss: 0.16700568795204163\n",
      "Epoch 53, loss: 0.14460425078868866\n",
      "Epoch 53, loss: 0.17197874188423157\n",
      "Epoch 53, loss: 0.09882514178752899\n",
      "Epoch 54, loss: 0.14063991606235504\n",
      "Epoch 54, loss: 0.11578190326690674\n",
      "Epoch 54, loss: 0.15909673273563385\n",
      "Epoch 55, loss: 0.12790405750274658\n",
      "Epoch 55, loss: 0.15720245242118835\n",
      "Epoch 55, loss: 0.1365991085767746\n",
      "Epoch 56, loss: 0.11239198595285416\n",
      "Epoch 56, loss: 0.13486972451210022\n",
      "Epoch 56, loss: 0.12775272130966187\n",
      "Epoch 57, loss: 0.11152290552854538\n",
      "Epoch 57, loss: 0.1494738608598709\n",
      "Epoch 57, loss: 0.12779684364795685\n",
      "Epoch 58, loss: 0.14170566201210022\n",
      "Epoch 58, loss: 0.12401439249515533\n",
      "Epoch 58, loss: 0.12719987332820892\n",
      "Epoch 59, loss: 0.12208200991153717\n",
      "Epoch 59, loss: 0.15061786770820618\n",
      "Epoch 59, loss: 0.12288819998502731\n",
      "Epoch 60, loss: 0.13364142179489136\n",
      "Epoch 60, loss: 0.13657446205615997\n",
      "Epoch 60, loss: 0.12335431575775146\n",
      "Epoch 61, loss: 0.11491449922323227\n",
      "Epoch 61, loss: 0.18612265586853027\n",
      "Epoch 61, loss: 0.11285372823476791\n",
      "Epoch 62, loss: 0.18144655227661133\n",
      "Epoch 62, loss: 0.15363629162311554\n",
      "Epoch 62, loss: 0.10605557262897491\n",
      "Epoch 63, loss: 0.14229485392570496\n",
      "Epoch 63, loss: 0.16962623596191406\n",
      "Epoch 63, loss: 0.10638519376516342\n",
      "Epoch 64, loss: 0.11494998633861542\n",
      "Epoch 64, loss: 0.13681772351264954\n",
      "Epoch 64, loss: 0.15747803449630737\n",
      "Epoch 65, loss: 0.11099094152450562\n",
      "Epoch 65, loss: 0.11204350739717484\n",
      "Epoch 65, loss: 0.13337844610214233\n",
      "Epoch 66, loss: 0.07430873811244965\n",
      "Epoch 66, loss: 0.1846432238817215\n",
      "Epoch 66, loss: 0.10241171717643738\n",
      "Epoch 67, loss: 0.14173226058483124\n",
      "Epoch 67, loss: 0.13990935683250427\n",
      "Epoch 67, loss: 0.10704801976680756\n",
      "Epoch 68, loss: 0.07966996729373932\n",
      "Epoch 68, loss: 0.14946497976779938\n",
      "Epoch 68, loss: 0.139476478099823\n",
      "Epoch 69, loss: 0.1973722130060196\n",
      "Epoch 69, loss: 0.11283662915229797\n",
      "Epoch 69, loss: 0.16446559131145477\n",
      "Epoch 70, loss: 0.12032001465559006\n",
      "Epoch 70, loss: 0.14154458045959473\n",
      "Epoch 70, loss: 0.14802870154380798\n",
      "Epoch 71, loss: 0.12948936223983765\n",
      "Epoch 71, loss: 0.13279901444911957\n",
      "Epoch 71, loss: 0.1414569765329361\n",
      "Epoch 72, loss: 0.11670354008674622\n",
      "Epoch 72, loss: 0.188631072640419\n",
      "Epoch 72, loss: 0.13480602204799652\n",
      "Epoch 73, loss: 0.09879559278488159\n",
      "Epoch 73, loss: 0.17844358086585999\n",
      "Epoch 73, loss: 0.12094012647867203\n",
      "Epoch 74, loss: 0.20922358334064484\n",
      "Epoch 74, loss: 0.1367362141609192\n",
      "Epoch 74, loss: 0.13319087028503418\n",
      "Epoch 75, loss: 0.5331021547317505\n",
      "Epoch 75, loss: 0.1448105275630951\n",
      "Epoch 75, loss: 0.1728493869304657\n",
      "Epoch 76, loss: 0.156510591506958\n",
      "Epoch 76, loss: 0.24261631071567535\n",
      "Epoch 76, loss: 0.09900622069835663\n",
      "Epoch 77, loss: 0.21425049006938934\n",
      "Epoch 77, loss: 0.1178777739405632\n",
      "Epoch 77, loss: 0.1340837925672531\n",
      "Epoch 78, loss: 0.14741647243499756\n",
      "Epoch 78, loss: 0.1483231782913208\n",
      "Epoch 78, loss: 0.12423683702945709\n",
      "Epoch 79, loss: 0.1154588833451271\n",
      "Epoch 79, loss: 0.1014401838183403\n",
      "Epoch 79, loss: 0.15677516162395477\n",
      "Epoch 80, loss: 0.11420173943042755\n",
      "Epoch 80, loss: 0.32270336151123047\n",
      "Epoch 80, loss: 0.1355745494365692\n",
      "Epoch 81, loss: 0.13762666285037994\n",
      "Epoch 81, loss: 0.13633204996585846\n",
      "Epoch 81, loss: 0.13737526535987854\n",
      "Epoch 82, loss: 0.12453220784664154\n",
      "Epoch 82, loss: 0.14253844320774078\n",
      "Epoch 82, loss: 0.15748122334480286\n",
      "Epoch 83, loss: 0.19348017871379852\n",
      "Epoch 83, loss: 0.10915777087211609\n",
      "Epoch 83, loss: 0.12115026265382767\n",
      "Epoch 84, loss: 0.12567880749702454\n",
      "Epoch 84, loss: 0.15371616184711456\n",
      "Epoch 84, loss: 0.1740596890449524\n",
      "Epoch 85, loss: 0.15775762498378754\n",
      "Epoch 85, loss: 0.15412300825119019\n",
      "Epoch 85, loss: 0.1296476274728775\n",
      "Epoch 86, loss: 0.16373851895332336\n",
      "Epoch 86, loss: 0.11828166991472244\n",
      "Epoch 86, loss: 0.1180446669459343\n",
      "Epoch 87, loss: 0.13226157426834106\n",
      "Epoch 87, loss: 0.18373164534568787\n",
      "Epoch 87, loss: 0.14235199987888336\n",
      "Epoch 88, loss: 0.15133704245090485\n",
      "Epoch 88, loss: 0.14185288548469543\n",
      "Epoch 88, loss: 0.11948122084140778\n",
      "Epoch 89, loss: 0.12272734940052032\n",
      "Epoch 89, loss: 0.12643055617809296\n",
      "Epoch 89, loss: 0.10729701071977615\n",
      "Epoch 90, loss: 0.14758314192295074\n",
      "Epoch 90, loss: 0.11877383291721344\n",
      "Epoch 90, loss: 0.13171906769275665\n",
      "Epoch 91, loss: 0.20236144959926605\n",
      "Epoch 91, loss: 0.14841453731060028\n",
      "Epoch 91, loss: 0.134426549077034\n",
      "Epoch 92, loss: 0.10846602916717529\n",
      "Epoch 92, loss: 0.15341496467590332\n",
      "Epoch 92, loss: 0.12721435725688934\n",
      "Epoch 93, loss: 0.14547228813171387\n",
      "Epoch 93, loss: 0.10796527564525604\n",
      "Epoch 93, loss: 0.1398433893918991\n",
      "Epoch 94, loss: 0.12818314135074615\n",
      "Epoch 94, loss: 0.12941493093967438\n",
      "Epoch 94, loss: 0.16904599964618683\n",
      "Epoch 95, loss: 0.11873331665992737\n",
      "Epoch 95, loss: 0.17997345328330994\n",
      "Epoch 95, loss: 0.14141954481601715\n",
      "Epoch 96, loss: 0.34955596923828125\n",
      "Epoch 96, loss: 0.29395273327827454\n",
      "Epoch 96, loss: 0.16791421175003052\n",
      "Epoch 97, loss: 0.11971359699964523\n",
      "Epoch 97, loss: 0.11589689552783966\n",
      "Epoch 97, loss: 0.16182009875774384\n",
      "Epoch 98, loss: 0.15109698474407196\n",
      "Epoch 98, loss: 0.1642252504825592\n",
      "Epoch 98, loss: 0.1182650700211525\n",
      "Epoch 99, loss: 0.12558981776237488\n",
      "Epoch 99, loss: 0.17462904751300812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, loss: 0.14072725176811218\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[0],\n",
    "        max_epochs=18,\n",
    "        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "        check_val_every_n_epoch=120,\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        precision=\"16-mixed\",\n",
    "        limit_val_batches=5,\n",
    "        num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does the image not contain a gray shape?\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD26iiiszcKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArkvEHxH8P+GdVbTtRkuFuFRXIjhLDB6c11tfN/xn/5KFN/17RfyNe9kOX0cfi/Y1r2s3pptYxrTcI3R6f/AMLn8If89rz/AMBzR/wufwh/z2vP/Ac1830V9x/qjl/eX3/8A4/rUz6Q/wCFz+EP+e15/wCA5rubC9h1LT7e9tyTDcRrIhYYO0jI4r42r638If8AImaL/wBeUX/oAr5niLJcNl1KE6F7ybWrv0v2OihWlUbTNqiiivjzqCvm/wCM/wDyUKb/AK9ov5GvpCvE/ib4D8R+IPGMl/pmn+fbGCNA/movIHPBNfT8L16VHHOdWSiuV6t27HPiYtwsjxqiu0/4VP4z/wCgR/5Hj/8AiqP+FT+M/wDoEf8AkeP/AOKr9M/tTA/8/o/+BI8/2c+xxdfW/hD/AJEzRf8Aryi/9AFfPn/Cp/Gf/QI/8jx//FV9E+G7Saw8M6XaXKbJ4LWOORc5wwUAivjOLcXh69CkqM1JpvZp9DqwsZRbuj//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAABJElEQVR4Ae2aMQ6CQBBFAQk2Bqw00RjPY2Fv423svBUJHsXCSmy0kYglE2LzljEk325M5s/Oe9BsiF+3azTmXzLmw3/PrgX+bVAGZAAS0CMEAeJ2GcAIYYAMQIC4XQYwQhggAxAgbpcBjBAGyAAEiNtlACOEATIAAeJ2GcAIYYAMQIC4XQYwQhggAxAgbpcBjBAGyAAEiNtTnNAJKC/Tsso6fwUq5kVzPDyLvDF5oReostN5ZmYEKbeb9373KnIbpnfAEvGuZcCbuJ0nA5aIdy0D3sTtPBmwRLxrGfAmbufJgCXiXcuAN3E7TwYsEe9aBryJ23kyYIl414GvVdrbm/b+Y4gl1qt32nfYOOzX63Wd3B/xEAukk2i56Nkh8AJDHP135gflrB34+KyJkgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image\n",
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "example = train_dataset[5]\n",
    "image = example[0]\n",
    "\n",
    "text_inputs = processor.tokenizer(\n",
    "    example[1][\"question\"], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "question = example[1]['question']\n",
    "\n",
    "text = f\"Question: {question} Answer:\"\n",
    "print(question)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          ...,\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281],\n",
       "          [1.8281, 1.8281, 1.8281,  ..., 1.8281, 1.8281, 1.8281]],\n",
       "\n",
       "         [[1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          ...,\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598],\n",
       "          [1.7598, 1.7598, 1.7598,  ..., 1.7598, 1.7598, 1.7598]],\n",
       "\n",
       "         [[1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          ...,\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900],\n",
       "          [1.7900, 1.7900, 1.7900,  ..., 1.7900, 1.7900, 1.7900]]]],\n",
       "       device='cuda:0', dtype=torch.float16), 'input_ids': tensor([[    2, 45641,    35,   473,     5,  2274,    45,  5585,    10, 12339,\n",
       "          3989,   116, 31652,    35]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to('cuda', torch.float16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yes.\n"
     ]
    }
   ],
   "source": [
    "# prepare image for the model\n",
    "import torch\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to('cuda', torch.float32).to('cuda', torch.float16)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "model.to('cuda')\n",
    "# generated_ids = model.generate(pixel_values, max_length=25)\n",
    "generated_ids = model.generate(**inputs)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
