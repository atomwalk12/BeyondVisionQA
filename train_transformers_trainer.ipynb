{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atomwalk12/Dropbox (Old)/notes/vision/project/BeyondVisionQA/model.py:128: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.metric = load_metric(\"bertscore\")\n",
      "/home/atomwalk12/anaconda3/envs/questllama/lib/python3.9/site-packages/datasets/load.py:756: FutureWarning: The repository for bertscore contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/bertscore/bertscore.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95017e401ca4450a6748a2e9996d972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b17c767ba124673afff8b38e5588242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38575 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from qadataset import QADataset\n",
    "from config import dataset_config, model_config\n",
    "\n",
    "train_dataset = QADataset(dataset_config, split=\"train[:6]\")\n",
    "val_dataset = QADataset(dataset_config, split=\"train[:6]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e689445108c48a0993880ed5d1c9021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,242,880 || all params: 3,749,922,816 || trainable%: 0.1398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Blip2ForConditionalGeneration(\n",
       "      (vision_model): Blip2VisionModel(\n",
       "        (embeddings): Blip2VisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "        )\n",
       "        (encoder): Blip2Encoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-38): 39 x Blip2EncoderLayer(\n",
       "              (self_attn): Blip2Attention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (qkv): Linear8bitLt(in_features=1408, out_features=4224, bias=True)\n",
       "                (projection): Linear8bitLt(in_features=1408, out_features=1408, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Blip2MLP(\n",
       "                (activation_fn): GELUActivation()\n",
       "                (fc1): Linear8bitLt(in_features=1408, out_features=6144, bias=True)\n",
       "                (fc2): Linear8bitLt(in_features=6144, out_features=1408, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (qformer): Blip2QFormerModel(\n",
       "        (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (encoder): Blip2QFormerEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (crossattention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (crossattention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (crossattention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (crossattention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (crossattention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (crossattention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=1408, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): Blip2QFormerLayer(\n",
       "              (attention): Blip2QFormerAttention(\n",
       "                (attention): Blip2QFormerMultiHeadAttention(\n",
       "                  (query): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): Blip2QFormerSelfOutput(\n",
       "                  (dense): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate_query): Blip2QFormerIntermediate(\n",
       "                (dense): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output_query): Blip2QFormerOutput(\n",
       "                (dense): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (language_projection): Linear8bitLt(in_features=768, out_features=2560, bias=True)\n",
       "      (language_model): OPTForCausalLM(\n",
       "        (model): OPTModel(\n",
       "          (decoder): OPTDecoder(\n",
       "            (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n",
       "            (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
       "            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (layers): ModuleList(\n",
       "              (0-31): 32 x OPTDecoderLayer(\n",
       "                (self_attn): OPTAttention(\n",
       "                  (k_proj): lora.Linear8bitLt(\n",
       "                    (base_layer): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2560, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
       "                  (q_proj): lora.Linear8bitLt(\n",
       "                    (base_layer): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=2560, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
       "                )\n",
       "                (activation_fn): ReLU()\n",
       "                (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "                (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
       "                (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
       "                (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from model import get_model\n",
    "\n",
    "# model = get_model(quantization='8bit')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from trainer_blip2 import BLIP2ModelPLModule\n",
    "from trainer_blip2 import BLIP2PLModule\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"ybelkada/blip2-opt-2.7b-fp16-sharded\", device_map=\"auto\", load_in_8bit=True)\n",
    "# Let's define the LoraConfig\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=BLIP2PLModule.train_textual_labels, batch_size=8, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614386e67cbe4ad68575b26cb132f6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883af51d70714a0891739ced0d618736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoTokenizer\n",
    "from dataset_configs.easy_vqa import translate\n",
    "from dataset_configs.easy_vqa import filter, get_image\n",
    "from PIL import Image\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"blip2-easyvqa-hf_trainer\", eval_strategy=\"epoch\", )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "train_dataset[0]\n",
    "def tokenize_function(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for i in range(len(examples['question'])):  # Iterate over the indices\n",
    "        item = {\n",
    "            'question': examples['question'][i],\n",
    "            'answer': examples['answer'][i],\n",
    "            'id': examples['id'][i],\n",
    "            'label': examples['label'][i],\n",
    "            'path': examples['path'][i]\n",
    "        }\n",
    "        texts.append(translate(item, training=True))\n",
    "        images.append(Image.open(examples['path'][i]))\n",
    "    return processor(images=images, text=texts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_datasets = train_dataset.dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = val_dataset.dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WUPMeasure' from partially initialized module 'model' (most likely due to a circular import) (/home/atomwalk12/Dropbox (Old)/notes/vision/project/BeyondVisionQA/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertScoreMetric\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m metric \u001b[38;5;241m=\u001b[39m BertScoreMetric()\n",
      "File \u001b[0;32m~/Dropbox (Old)/notes/vision/project/BeyondVisionQA/model.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, f1_score\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_config\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n",
      "File \u001b[0;32m~/Dropbox (Old)/notes/vision/project/BeyondVisionQA/config.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124measy-vqa\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset_configs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01measy_vqa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WUPMeasure, F1ScoreMetric, AccuracyMetric, BertScoreMetric\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01measy_vqa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_answers\n\u001b[1;32m     32\u001b[0m     dataset_config\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_fn\u001b[39m\u001b[38;5;124m'\u001b[39m: load_data}) \n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'WUPMeasure' from partially initialized module 'model' (most likely due to a circular import) (/home/atomwalk12/Dropbox (Old)/notes/vision/project/BeyondVisionQA/model.py)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from model import BertScoreMetric\n",
    "import numpy as np\n",
    "\n",
    "metric = BertScoreMetric()\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, lang='en')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, inputs in enumerate(train_dataloader):\n",
    "    input, labels = inputs\n",
    "    \n",
    "    outputs = model(**input,\n",
    "                labels=labels)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "example = train_dataset[5]\n",
    "image = example[0]\n",
    "\n",
    "text_inputs = processor.tokenizer(\n",
    "    example[1][\"question\"], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "question = example[1]['question']\n",
    "\n",
    "text = f\"Question: {question} Answer:\"\n",
    "print(text)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image for the model\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device, torch.float16)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "# generated_ids = model.generate(pixel_values, max_length=25)\n",
    "generated_ids = model.generate(**inputs, max_length=25)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "questllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
